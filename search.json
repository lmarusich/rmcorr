[{"path":"/articles/CI_fix.html","id":"confidence-interval-fix-in-0-5-4","dir":"Articles","previous_headings":"","what":"Confidence Interval Fix in 0.5.4","title":"Confidence Interval Fix","text":"versions rmcorr prior 0.5.4, confidence intervals (CIs) incorrectly calculated using error degrees freedom instead using effective sample size (error df + 2). Generally, difference slight, previous incorrect CIs tending bit broad (conservative). However, number participants repeated measures small, prior incorrect confidence intervals substantially wide. words, difference 2 df diminishing impact width CIs error df increases. fix change effect size, p-value, error degrees freedom. remain prior versions. Bootstrapped CIs also unaffected.","code":""},{"path":"/articles/CI_fix.html","id":"example-with-slight-difference-in-cis","dir":"Articles","previous_headings":"Confidence Interval Fix in 0.5.4","what":"Example with Slight Difference in CIs","title":"Confidence Interval Fix","text":"use data raz2005 compare old, incorrect CI calculation current, correct CI. sample size N = 72 k = 2 repeated measures. Note difference CI bounds slight: old, incorrect 95% CI [-0.8053581, -0.5637514] current, correct 95% CI [ -0.804153, -0.566080].","code":"brainvolage.rmc <- rmcorr(participant = Participant, measure1 = Age, measure2 = Volume, dataset = raz2005) #> Warning in rmcorr(participant = Participant, measure1 = Age, measure2 = Volume, #> : 'Participant' coerced into a factor  #Old, incorrect CI using error df psych::r.con(brainvolage.rmc$r, brainvolage.rmc$df, p = 0.95) #> [1] -0.8053581 -0.5637514  #Current, correct CI using effective sample size brainvolage.rmc$CI #> [1] -0.804153 -0.566080  #Manual calculation psych::r.con(brainvolage.rmc$r, brainvolage.rmc$df + 2, p = 0.95) #> [1] -0.804153 -0.566080"},{"path":"/articles/CI_fix.html","id":"example-with-substantial-difference-in-cis","dir":"Articles","previous_headings":"Confidence Interval Fix in 0.5.4","what":"Example with Substantial Difference in CIs","title":"Confidence Interval Fix","text":"small sample size (N = 5 total participants) k = 2 repeated measures rrm = 0.88, show substantial difference old 95% CI [-0.5257087, 0.9974701] compared correct 95% CI [0.2394418, 0.9868084].","code":"#Old, incorrect CI using error df psych::r.con(0.88, 4, p = 0.95) #> [1] -0.5257087  0.9974701  #Current, correct CI using effective sample size psych::r.con(0.88, 6, p = 0.95) #> [1] 0.2394418 0.9868084"},{"path":"/articles/CI_fix.html","id":"another-slight-example-same-as-above-with-a-larger-sample-size","dir":"Articles","previous_headings":"Confidence Interval Fix in 0.5.4","what":"Another Slight Example: Same as Above with a Larger Sample Size","title":"Confidence Interval Fix","text":"example except number participants quadrupled: N = 20. Note difference coverage CIs now slight.","code":"#Old, incorrect CI using error df psych::r.con(0.88, 19, p = 0.95) #> [1] 0.7093015 0.9532081  #Current, correct CI using effective sample size psych::r.con(0.88, 21, p = 0.95) #> [1] 0.7229510 0.9505773"},{"path":[]},{"path":"/articles/FAQ_and_limitations.html","id":"how-to-calculate-power","dir":"Articles","previous_headings":"Frequently Asked Questions","what":"How to Calculate Power","title":"Frequently Asked Questions and Limitations","text":"Power can calculated using power.rmcorr function. function modifies pwr.r.test pwr package use rmcorr degrees freedom. presently included rmcorr package. Notation: N sample size, k (average) number repeated measures individual, rrm rmcorr effect size.","code":""},{"path":"/articles/FAQ_and_limitations.html","id":"power-rmcorr-example","dir":"Articles","previous_headings":"Frequently Asked Questions > How to Calculate Power","what":"power.rmcorr Example","title":"Frequently Asked Questions and Limitations","text":"N = 100, k = 3, rrm = 0.20. design 82% power. See Power curves information. G*Power (Faul et al. 2009) software, power can calculated substituting rmcorr degrees freedom Pearson correlation. Instead sample size, use degrees freedom rmcorr plus two (effective sample size). Pearson correlation N - 2 degrees freedom. Rmcorr exact degrees freedom = N x (k - 1) - 1 Approximate degrees freedom = (N -1) x (k - 1)","code":"install.packages(\"pwr\") require(pwr)  power.rmcorr <- function(k, N, effectsizer, sig)     {pwr.r.test(n = ((N)*(k-1))+1, r = effectsizer, sig.level = sig)}      power.rmcorr(k = 3, N = 100, effectsizer = 0.20, sig = 0.05)     #>  #>      approximate correlation power calculation (arctangh transformation)  #>  #>               n = 201 #>               r = 0.2 #>       sig.level = 0.05 #>           power = 0.8156984 #>     alternative = two.sided"},{"path":"/articles/FAQ_and_limitations.html","id":"exact-degrees-of-freedom-example","dir":"Articles","previous_headings":"Frequently Asked Questions > How to Calculate Power","what":"Exact Degrees of Freedom Example","title":"Frequently Asked Questions and Limitations","text":"Values: N = 100, k = 3 , rrm = 0.20 ()  rmcorr df = 100 x (3 - 1) - 1 = 200 - 1 = 199  Add two (kludge): 199 + 2 = 201  Enter N = 201 effective sample size G*Power  G*Power calculate degrees freedom N - 2 = 201 - 2 = 199. Thus, using correct degrees freedom rmcorr. Note power just slighty different calculation R, uses arctan approximation.","code":""},{"path":"/articles/FAQ_and_limitations.html","id":"approximate-degrees-of-freedom-example","dir":"Articles","previous_headings":"Frequently Asked Questions > How to Calculate Power","what":"Approximate Degrees of Freedom Example","title":"Frequently Asked Questions and Limitations","text":"Values: N = 100, k = 3, rrm = 0.20 (, )  approx rmcorr df = (100 - 1) x (3 - 1) = 99 x 2 = 198  Note small difference approximate vs. exact calculation: one degree freedom  Add two (kludge) G*Power, entering sample size N = 200 Note power slighty different exact formula .","code":""},{"path":"/articles/FAQ_and_limitations.html","id":"how-to-extract-the-slope-and-its-confidence-interval","dir":"Articles","previous_headings":"Frequently Asked Questions","what":"How to Extract the Slope and its Confidence Interval","title":"Frequently Asked Questions and Limitations","text":"","code":"my.rmc <- rmcorr(participant = Subject, measure1 = PaCO2, measure2 = pH,                   dataset = bland1995) #> Warning in rmcorr(participant = Subject, measure1 = PaCO2, measure2 = pH, : #> 'Subject' coerced into a factor   # Structure of rmcorr object #str(my.rmc)   # Extract rmcorr model coefficients coef.rmc  <- my.rmc$model$coefficients coef.rmc #>  (Intercept) Participant1 Participant2 Participant3 Participant4 Participant5  #>   7.65590848  -0.72605418  -0.02144291   0.22395850   0.24550355   0.13432752  #> Participant6 Participant7     Measure1  #>   0.20037424  -0.03394863  -0.10832305   slope.rmc <- coef.rmc[length(coef.rmc)] #Last value in coefficients is the slope slope.rmc #>  Measure1  #> -0.108323   # Confidence intervals around all estimates coef.CIs <- stats::confint(my.rmc$model)  coefs.all <- cbind(coef.rmc, coef.CIs) coefs.all #>                 coef.rmc       2.5 %      97.5 % #> (Intercept)   7.65590848  7.34994594  7.96187102 #> Participant1 -0.72605418 -0.83211999 -0.61998837 #> Participant2 -0.02144291 -0.11116705  0.06828123 #> Participant3  0.22395850  0.16049969  0.28741732 #> Participant4  0.24550355  0.16448903  0.32651806 #> Participant5  0.13432752  0.05868538  0.20996967 #> Participant6  0.20037424  0.12673292  0.27401556 #> Participant7 -0.03394863 -0.17148111  0.10358385 #> Measure1     -0.10832305 -0.16883787 -0.04780822"},{"path":"/articles/FAQ_and_limitations.html","id":"isa-error","dir":"Articles","previous_headings":"Frequently Asked Questions","what":"isa Error","title":"Frequently Asked Questions and Limitations","text":"reports error running rmcorr: Stack Question error Updating R version 4.1.0 later resolves error.","code":"Error in isa(Participant, \"character\") : could not find function \"isa\""},{"path":"/articles/FAQ_and_limitations.html","id":"transformations","dir":"Articles","previous_headings":"Frequently Asked Questions","what":"Transformations","title":"Frequently Asked Questions and Limitations","text":"Transformations can used make data (errors) normal. highly recommend graphing raw transformed data. may appropriate transform one measure transform measures. “Consider transforming every variable sight” (Gelman Hill 2007, 548).","code":""},{"path":[]},{"path":"/articles/FAQ_and_limitations.html","id":"change-over-time","dir":"Articles","previous_headings":"Limitations","what":"Change Over Time","title":"Frequently Asked Questions and Limitations","text":"general, rmcorr time-independent model– model change time. partial exception time measure, age raz2005 dataset.","code":""},{"path":"/articles/FAQ_and_limitations.html","id":"non-linearity","dir":"Articles","previous_headings":"Limitations","what":"Non-Linearity","title":"Frequently Asked Questions and Limitations","text":"Rmcorr fits linear model. data non-linear, recommend trying transform (see ) using multilevel modeling. Also see Diagnostic Plots","code":""},{"path":"/articles/FAQ_and_limitations.html","id":"varying-slopes-with-influential-observations-andor-unbalanced-data","dir":"Articles","previous_headings":"Limitations","what":"Varying Slopes with Influential Observations and/or Unbalanced Data","title":"Frequently Asked Questions and Limitations","text":"slopes meaningfully vary individual, recommend using multilevel modeling instead rmcorr. Random effect slopes even problematic rmcorr influential observations /highly unbalanced data. nicely illustrated simulations Dr. Marta Karas: rmcorr may ideal","code":""},{"path":"/articles/FAQ_and_limitations.html","id":"other-implementations-of-rmcorr","dir":"Articles","previous_headings":"","what":"Other Implementations of rmcorr","title":"Frequently Asked Questions and Limitations","text":"know three implementations rmcorr.","code":""},{"path":"/articles/FAQ_and_limitations.html","id":"rmcorrshiny-web-and-standalone-app-with-a-graphical-interface-marusich2021rmcorrshiny","dir":"Articles","previous_headings":"Other Implementations of rmcorr","what":"1) rmcorrShiny: Web and Standalone App with a Graphical Interface (Marusich and Bakdash 2021)","title":"Frequently Asked Questions and Limitations","text":"Web App Shiny Source Code Running Standalone App","code":""},{"path":"/articles/FAQ_and_limitations.html","id":"python-rm_corr-in-pingouin-vallat2018pingouin","dir":"Articles","previous_headings":"Other Implementations of rmcorr","what":"2) Python: rm_corr in Pingouin (Vallat 2018)","title":"Frequently Asked Questions and Limitations","text":"rm_corr","code":""},{"path":"/articles/FAQ_and_limitations.html","id":"stata-rmcorr-stata-rmcorr","dir":"Articles","previous_headings":"Other Implementations of rmcorr","what":"3) Stata: RMCORR (Linden 2021)","title":"Frequently Asked Questions and Limitations","text":"RMCORR","code":""},{"path":"/articles/New_rmcorr_paper_analyses_figures.html","id":"required-packages-and-functions","dir":"Articles","previous_headings":"","what":"Required Packages and Functions","title":"Reproduce Paper Results","text":"","code":"if (!require(\"pacman\")) install.packages(\"pacman\") pacman::p_load(pwr, psych, RColorBrewer, plotrix, rmcorr, lme4, ggplot2,                 AICcmodavg, ggplot2, merTools, pals)  #Get the working directory workingdir <- getwd() dir.create(file.path(workingdir, \"/plots\"), showWarnings = F)   pvals.fct <- function(input) {   input <- ifelse(input < 0.001, \"< 0.001\",                   ifelse(input < 0.01, \"< 0.01\",                          ifelse(input < 0.05 & input > 0.04, \"< 0.05\",                                 ifelse(round(input, 2) == 1.00, \"0.99\",                                        sprintf(\"%.2f\", round(input, 2)))                          )                     )                      ) }   addalpha <- function(colors, alpha=1.0) {   r <- col2rgb(colors, alpha=T)   # Apply alpha   r[4,] <- alpha*255   r <- r/255.0   return(rgb(r[1,], r[2,], r[3,], r[4,])) }  colorRampPaletteAlpha <- function(colors, n = 32, interpolate='linear') {   # Create the color ramp normally   cr <- colorRampPalette(colors, interpolate=interpolate)(n)   # Find the alpha channel   a <- col2rgb(colors, alpha=T)[4,]   # Interpolate   if (interpolate=='linear') {     l <- approx(a, n=n)   } else {     l <- spline(a, n=n)   }   l$y[l$y > 255] <- 255 # Clamp if spline is > 255   cr <- addalpha(cr, l$y/255.0)   return(cr) }  #Not required  num.cpus <- parallel::detectCores(logical = T) options(boot.ncpus = num.cpus)  #Number of cpus R is using getOption(\"boot.ncpus\", 1L)"},{"path":"/articles/New_rmcorr_paper_analyses_figures.html","id":"figure-1-rmcorr-and-reg-plot","dir":"Articles","previous_headings":"","what":"1. Figure 1: rmcorr and reg plot","title":"Reproduce Paper Results","text":"","code":"# echo = FALSE, warning = FALSE, results =  \"hide\", set.seed(1)  initX <- rnorm(50) newY <- NULL newX <- NULL sub <- rep(1:10, each = 5)  rsq <- .9  addx <- -2 for (i in 1:10){     addx <- addx + .25     tempData <- initX[sub == i] + addx     sdx <- sd(tempData)     sdnoise <- sdx * (sqrt((1-rsq)/rsq))     tempy <- tempData + rnorm(5,0,sdnoise) + rnorm(1,0,3)     newY <- c(newY, tempy)     newX <- c(newX,tempData) }  exampleMat <-data.frame(cbind(sub,newX,newY))  ###standard averaged regression plot submeanx <- aggregate(exampleMat$newX, by = list(exampleMat$sub), mean) submeany <- aggregate(exampleMat$newY, by = list(exampleMat$sub), mean) mypal <- colorRampPalette(RColorBrewer::brewer.pal(10,'Paired')) cols <- mypal(10)  example.rmc <- rmcorr(sub,newX,newY,exampleMat) #> Warning in rmcorr(sub, newX, newY, exampleMat): 'sub' coerced into a factor  #for graphing: get the rmcorr coefficient (rounded) and p-value (using pvals.fct) example.rmc.r <- sprintf(\"%.2f\", round(example.rmc$r, 2)) example.rmc.p <- pvals.fct(example.rmc$p)  #ditto for cor stdr <- cor.test(submeanx[,2], submeany[,2]) example.cor.r <- sprintf(\"%.2f\", round(stdr$estimate, 2)) example.cor.p <- pvals.fct(stdr$p.value)     par(mfrow = c(1, 2), mgp = c(2.5, .75, 0), mar = c(4,4,2,1), cex = 1.2)  plot(example.rmc, xlab = \"x\", ylab = \"y\",      overall = F, palette = mypal, las = 1, ylim = c(-6, 6.5)) title(\"A)\", adj = 0) #Removed for Frontiers formatting  text(1.25, -5, adj = 1, bquote(italic(r[rm])~\"=\"~ .(example.rmc.r))) text(1.25, -5.75, adj = 1, bquote(italic('p')~.(example.rmc.p)))  plot(submeanx[,2], submeany[,2], pch = 16, col = cols, las = 1,      xlab = \"x (averaged for each participant)\",      ylab = \"y (averaged for each participant)\", ylim=c(-6,6.5), xlim=c(-3, 1)) title(\"B)\", adj = 0) #  text(0.90, -5, adj = 1, bquote(italic('r')~\"=\"~ .(example.cor.r))) text(0.90, -5.75, adj = 1, bquote(italic('p')~\"=\"~.(example.cor.p)))  abline(lm(submeany[,2]~submeanx[,2]),col=\"gray50\") #(A) Rmcorr plot: rmcorr plot for a set of hypothetical data and (B) simple regression plot: the  #corresponding regression plot for the same data averaged by participant.  #dev.copy2eps(file=\"plots/Figure1_Rmcorr_vs_reg.eps\", height = 6, width = 8) #dev.copy(pdf, file=\"plots/Figure1_Rmcorr_vs_reg.pdf\", height = 6, width = 8) #dev.off()"},{"path":"/articles/New_rmcorr_paper_analyses_figures.html","id":"figure-2-rmcorr-vs-ols-reg","dir":"Articles","previous_headings":"","what":"2. Figure 2: rmcorr vs OLS reg","title":"Reproduce Paper Results","text":"","code":"par(mfrow = c(3,3), mar = c(1,1,.5,.5), mgp = c(2.5,.75,0),      oma = c(4,4,4,0), cex = 1.1)  makeminiplot <- function(subxs, sub.slope, intercept, constant=0, xax = \"n\",                           yax = \"n\", legend = F){          mypal <- colorRampPalette(RColorBrewer::brewer.pal(10,'Paired'))     cols <- mypal(3)          # cols <- c(\"#A6CEE3\", \"#9D686D\", \"#6A3D9A\")          subys <- list(3)     for (i in 1:3){         subys[[i]] <- subxs[[i]] * sub.slope + intercept*i + constant     }          plot(subxs[[1]],subys[[1]], type = \"n\", xlim =c(0,4), ylim = c(0,10),           xlab = \"\", ylab = \"\", xaxt = xax, yaxt = yax, las = 1)          allx <- unlist(subxs)     ally <- unlist(subys)     abline(lm(ally~allx))          for (i in 1:3) {         lines(subxs[[i]],subys[[i]], type = \"o\", col = cols[i], pch = 16)     }          if (legend) legend('bottomright', legend = \"OLS\", lwd = 1.25, bty = \"n\",                        cex = 1, inset = -0.03) }  subxs <- list(3) subxs[[1]] <- seq(0,2,.25) subxs[[2]] <- seq(1,3,.25) subxs[[3]] <- seq(2,4,.25)  #ols is positive makeminiplot(subxs, -1, 4, yax = \"s\", legend = T) makeminiplot(subxs, 0, 2.75) makeminiplot(subxs, 1, 1.5)  #ols is flat makeminiplot(subxs, -1.5, 2.45, 3, yax = \"s\") makeminiplot(subxs, 0, 0, 5) makeminiplot(subxs, 1.5, -2.4, 7)  #ols is negative makeminiplot(subxs, -.75, -2, 10, yax = \"s\", xax = \"s\") makeminiplot(subxs, 0, -3.1, 10.9, xax = \"s\") makeminiplot(subxs, .9, -4.6, 12, xax = \"s\")  mtext(side = 1, outer = T, line = 1.5, \"x\", at = c(.175, .5, .85)) mtext(side = 2, outer = T, line = 1.5, \"y\", at = c(.175, .5, .85), las = 1) # mtext(side = 3, outer = T, line = .5,  #       c(\"a) rmcorr = -1\", \"b) rmcorr = 0\", \"c) rmcorr = 1\"), #       at = c(.175, .5, .85), las = 1, cex = 1.5)  #Figure 2. These notional plots illustrate the range of potential similarities and differences  #in the intra-individual association assessed by rmcorr and the inter-individual association  #assessed by ordinary least squares (OLS) regression. Rmcorr-values depend only on the  #intra-individual association between variables and will be the same across different patterns  #of inter-individual variability. (A) rrm = −1: depicts notional data with a perfect negative  #intra-individual association between variables, (B) rrm = 0: depicts data with no  #intra-individual association, and (C) rrm = 1: depicts data with a perfect positive  #intra-individual association. In each column, the relationship between subjects  #(inter-individual variability) is different, which does not change the rmcorr-values within a  #column. However, this does change the association that would be predicted by OLS regression  #(black lines) if the data were treated as IID or averaged by participant.  #dev.copy2eps(file=\"plots/Figure2_Rmcorr_vs_OLS.eps\", height = 8, width = 8) #dev.copy(pdf, file=\"plots/Figure2_Rmcorr_vs_OLS.pdf\", height = 8, width = 8) #dev.off()"},{"path":"/articles/New_rmcorr_paper_analyses_figures.html","id":"figure-3-rmcorr-wdata-transformations","dir":"Articles","previous_headings":"","what":"3. Figure 3: rmcorr w/data transformations","title":"Reproduce Paper Results","text":"","code":"set.seed(10) initX <- rnorm(15) newY <- NULL newX <- NULL sub <- rep(1:3, each = 5) rsq <- .7 addy <- 4 addx <- -2 for (i in 1:3){     addy <- addy - 1     addx <- addx + .25          tempData <- initX[sub == i] + addx     sdx <- sd(tempData)     sdnoise <- sdx * (sqrt((1-rsq)/rsq))     tempy <- tempData + rnorm(5,0,sdnoise) + rnorm(1,addy,1)     newY <- c(newY, tempy)     newX <- c(newX,tempData) }  par(mfrow=c(1,3), mar = c(4,4,2,2), mgp = c(2.75, .75, 0), cex = 1.2)  ###original plot exampleMat <-data.frame(cbind(sub,newX,newY)) example1.rmc <- rmcorr(sub,newX,newY,exampleMat) #> Warning in rmcorr(sub, newX, newY, exampleMat): 'sub' coerced into a factor  mypal <- colorRampPalette(RColorBrewer::brewer.pal(10,'Paired'))  plot(example1.rmc, xlab = \"x\", ylab = \"\",       overall = F, palette = mypal, xlim = c(-3.5, 1), ylim = c(-2.5,2), las = 1)  example1.rmc.r <- sprintf(\"%.2f\", round(example1.rmc$r, 2)) example1.rmc.p <- pvals.fct(example1.rmc$p)  text(-3.5, 2, adj = 0, bquote(italic(r[rm])~\"=\"~ .(example1.rmc.r))) text(-3.5, 1.75, adj = 0, bquote(italic('p')~.(example1.rmc.p))) mtext(side = 2, \"y\", las = 1, line = 2.5, cex = 1.2)  ###add 1 to all x's, multiply by 2 exampleMat2 <- exampleMat exampleMat2$newX <- exampleMat2$newX * .5 + 1 example2.rmc <- rmcorr(sub, newX, newY, exampleMat2) #> Warning in rmcorr(sub, newX, newY, exampleMat2): 'sub' coerced into a factor  example2.rmc.r <- sprintf(\"%.2f\", round(example2.rmc$r, 2)) example2.rmc.p <- pvals.fct(example2.rmc$p)  plot(example2.rmc, xlab = \"x\", ylab = \"\", overall = F,      palette = mypal, xlim = c(-3.5, 1), ylim = c(-2.5,2), las = 1)  text(-3.5, 2, adj = 0, bquote(italic(r[rm])~\"=\"~ .(example2.rmc.r))) text(-3.5, 1.75, adj = 0, bquote(italic('p')~.(example2.rmc.p)))  mtext(side = 2, \"y\", las = 1, line = 2.5, cex = 1.2)  ###just add -2 to sub3's ys exampleMat3 <- exampleMat exampleMat3$newY[11:15] <- exampleMat3$newY[11:15] - 2 example3.rmc <- rmcorr(sub, newX, newY, exampleMat3) #> Warning in rmcorr(sub, newX, newY, exampleMat3): 'sub' coerced into a factor  example3.rmc.r <- sprintf(\"%.2f\", round(example3.rmc$r, 2)) example3.rmc.p <- pvals.fct(example3.rmc$p)  plot(example3.rmc, xlab = \"x\", ylab = \"\", overall = F,      palette = mypal, xlim = c(-3.5, 1), ylim = c(-2.5,2), las = 1)  text(-3.5, 2, adj = 0, bquote(italic(r[rm])~\"=\"~ .(example3.rmc.r))) text(-3.5, 1.75, adj = 0, bquote(italic('p')~.(example3.rmc.p)))  mtext(side = 2, \"y\", las = 1, line = 2.5, cex = 1.2) #Figure 3. Rmcorr-values (and corresponding p-values) do not change with linear  #transformations of the data, illustrated here with three examples: (A) original, (B) x/2 + 1, and (C) y − 1.  #dev.copy2eps(file=\"plots/Figure3_Transformations.eps\", height = 6, width = 12) #dev.copy(pdf, file=\"plots/Figure3_Transformations.pdf\", height = 6, width = 12) #dev.off()"},{"path":"/articles/New_rmcorr_paper_analyses_figures.html","id":"power","dir":"Articles","previous_headings":"","what":"4. Figure 4: Power curves","title":"Reproduce Paper Results","text":"","code":"power.rmcorr<-function(k, N, effectsizer, sig) {     pwr.r.test(n = ((N)*(k-1))+1, r = effectsizer, sig.level = sig)      #df are specified this way because pwr.r.test assumes the input is N, so it uses N - 2 for the df }  par(mfrow=c(1,3), cex.lab=1.50, cex.axis=1.40, cex.sub=1.40, mar=c(4.5,4.5,1.75,1))  #Small effect size k<-c(3, 5, 10, 20)  nvals <- seq(6, 300) powPearsonSmall <- sapply(nvals, function (x) pwr.r.test(n=x, r=0.1)$power)  bluecolors<-c(\"#c6dbef\", \"#9ecae1\", \"#6baed6\", \"#4292c6\", \"#2171b5\", \"#084594\")   plot(nvals, seq(0,1, length.out=length(nvals)),       xlab=expression(Sample~Size~\"(\"*italic('N')*\")\"),      yaxt = \"n\", ylab = \"Power\", las = 1, col = \"white\",       xlim=c(0,300))  axis(1, at = seq(0, 300, 100)) yLabels <- seq(0, 1, 0.2) axis(2, at=yLabels, labels=sprintf(round(100*yLabels), fmt=\"%2.0f%%\"), las=1, cex.sub = 2)  for (i in 1:4)  {     powvals <- sapply(nvals, function (x) power.rmcorr(k[i], x, 0.1, 0.05)$power)     lines(nvals, powvals, lwd=2.5, col=bluecolors[i+1]) } legend(\"bottomright\", lwd=2.5, col=bluecolors, bty= 'n', legend=c(\"1\", \"3\", \"5\", \"10\", \"20\"), title = expression(italic('k')),        cex = 1.2) lines(nvals, powPearsonSmall, col=bluecolors[1], lwd= 2.5) abline(a = 0.8, b=0, col=1, lty=2, lwd= 2.5)  #Medium effect size k<-c(3, 5, 10, 20) nvals <- seq(6, 60) powPearsonMedium <- sapply(nvals, function (x) pwr.r.test(n=x, r=0.3)$power) greencolors<-c(\"#c7e9c0\",\"#a1d99b\",\"#74c476\",\"#41ab5d\",\"#238b45\",\"#005a32\")  #orangecols<-brewer.pal(9, \"Oranges\") #orangecols3<-c(orangecols[2],orangecols[3],orangecols[5],orangecols[7],orangecols[9])  plot(nvals, seq(0,1, length.out=length(nvals)),       xlab=expression(Sample~Size~\"(\"*italic('N')*\")\"),      yaxt = \"n\", ylab = \"Power\", las = 1, col = \"white\",       xlim=c(0,60))  axis(1, at = seq(0, 60, 20)) yLabels <- seq(0, 1, 0.2) axis(2, at=yLabels, main = \"Power\", labels=sprintf(round(100*yLabels), fmt=\"%2.0f%%\"), las=1)  for (i in 1:4)  {     powvals <- sapply(nvals, function (x) power.rmcorr(k[i], x, 0.3, 0.05)$power)     lines(nvals, powvals, lwd=2.5, col=greencolors[i+1]) } legend(\"bottomright\", lwd=2, col=greencolors, bty = 'n', legend=c(\"1\", \"3\", \"5\", \"10\", \"20\"), title = expression(italic('k')),        cex = 1.2) lines(nvals, powPearsonMedium, col=greencolors[1], lwd = 2.5) abline(a = 0.8, b=0, col=1, lty=2, lwd= 2.5)  #Large effect size k<-c(3, 5, 10, 20) nvals <- seq(6, 30) powPearsonlarge <- sapply(nvals, function (x) pwr.r.test(n=x, r=0.5)$power)  purplecolors<-c(\"#f2f0f7\", \"#dadaeb\", \"#bcbddc\", \"#9e9ac8\", \"#807dba\", \"#6a51a3\", \"#4a1486\")  plot(nvals, seq(0,1, length.out=length(nvals)),       xlab=expression(Sample~Size~\"(\"*italic('N')*\")\"),      yaxt = \"n\", ylab = \"Power\", las = 1, col = \"white\", xlim=c(0,30)) axis(1, at = seq(0, 40, 10)) yLabels <- seq(0, 1, 0.2) axis(2, at=yLabels, main = \"Power\", labels=sprintf(round(100*yLabels), fmt=\"%2.0f%%\"), las=1)  for (i in 1:4)  {     powvals <- sapply(nvals, function (x) power.rmcorr(k[i], x, 0.5, 0.05)$power)     lines(nvals, powvals, lwd=2.5, col=purplecolors[i+2]) } legend(\"bottomright\", lwd=2, col=purplecolors, legend=c(\"1\", \"3\", \"5\", \"10\", \"20\"), bty = 'n', title = expression(italic('k')),        cex = 1.2) abline(a = 0.8, b=0, col=1, lty=2, lwd= 2.5) lines(nvals, powPearsonlarge, col=purplecolors[2], lwd = 2.5) #Figure 4. Power curves for (A) small, rrm, and r = 0.10, (B) medium, rrm, and r = 0.3, and (C) large effect sizes, rrm,  #and r = 0.50. X-axis is sample size. Note the sample size range differs among the panels. Y-axis is power. k denotes  #the number of repeated paired measures. Eighty percent power is indicated by the dotted black line. For rmcorr, the power of  #k = 2 is asymptotically equivalent to k = 1. A comparison to the power for a Pearson correlation with one data point per  #participant (k = 1) is also shown.  #dev.copy2eps(file=\"plots/Figure4_Power_curves.eps\", height = 6, width = 6) #dev.copy(pdf, file=\"plots/Figure4_Power_curve.pdf\", height = 6, width = 6) #dev.off()"},{"path":[]},{"path":"/articles/New_rmcorr_paper_analyses_figures.html","id":"rmcorr-and-simple-reg-results","dir":"Articles","previous_headings":"5. Brain volume and age rmcorr and simple reg/cor results and Figure 5","what":"rmcorr and simple reg results","title":"Reproduce Paper Results","text":"","code":"#Note for details on Raz: Data captured from Figure 8, Cerebellar Hemispheres (lower right) #a) Reproduce correlations in the paper: Cross-sectional (correlation at Time 1) Time1raz2005<-subset(raz2005, Time == 1) Time2raz2005<-subset(raz2005, Time == 2) a1.rtest <- cor.test(Time1raz2005$Age, Time1raz2005$Volume)  a2.rtest <- cor.test(Time2raz2005$Age, Time2raz2005$Volume)  a1.lm <- lm(Time1raz2005$Volume ~ Time1raz2005$Age) a2.lm <- lm(Time2raz2005$Volume ~ Time2raz2005$Age)  summary.a1.lm <- summary(a1.lm) summary.a2.lm <- summary(a2.lm)  a1.lm.r <- sprintf(\"%.2f\", round(a1.rtest$estimate, 2)) #Same as Pearson correlation for simple regression a1.lm.p <- pvals.fct(summary.a1.lm$coefficients[2,4])  a2.lm.r <- sprintf(\"%.2f\", round(a2.rtest$estimate ,2)) #Same as Pearson correlation for simple regression a2.lm.p <- pvals.fct(summary.a2.lm$coefficients[2,4])  #b) rmcorr analysis brainvolage.rmc <- rmcorr(participant = Participant, measure1 = Age, measure2 = Volume, dataset = raz2005) #> Warning in rmcorr(participant = Participant, measure1 = Age, measure2 = Volume, #> : 'Participant' coerced into a factor print(brainvolage.rmc) #>  #> Repeated measures correlation #>  #> r #> -0.7044077 #>  #> degrees of freedom #> 71 #>  #> p-value #> 3.561007e-12 #>  #> 95% confidence interval #> -0.804153 -0.56608  rmcorr.5b.r <- sprintf(\"%.2f\", round(brainvolage.rmc$r, 2)) rmcorr.5b.p <- pvals.fct(brainvolage.rmc$p)  #c) simple regression on averaged data  avgRaz2005 <- aggregate(raz2005[,3:4], by = list(raz2005$Participant), mean) avg.lm <- lm(Volume~Age, data = avgRaz2005) summary.av.lm <- summary(avg.lm) c.rtest <- cor.test(avgRaz2005$Age, avgRaz2005$Volume) print(c.rtest) #>  #>  Pearson's product-moment correlation #>  #> data:  avgRaz2005$Age and avgRaz2005$Volume #> t = -3.4912, df = 70, p-value = 0.000837 #> alternative hypothesis: true correlation is not equal to 0 #> 95 percent confidence interval: #>  -0.5662456 -0.1684542 #> sample estimates: #>        cor  #> -0.3850943  fig.5c.r <- sprintf(\"%.2f\", round(c.rtest$estimate,2)) fig.5c.p <-  pvals.fct(summary.av.lm$coefficients[2,4])  #Not graphed in Figure 5 #d) simple regression on aggregated data (incorrect overfit model): #Although in this case it doesn't matter   brainvolage.lm<-lm(Volume~Age, data = raz2005) print(brainvolage.lm) #>  #> Call: #> lm(formula = Volume ~ Age, data = raz2005) #>  #> Coefficients: #> (Intercept)          Age   #>    151.9068      -0.3399  d.rtest <- cor.test(raz2005$Age, raz2005$Volume) print(d.rtest) #>  #>  Pearson's product-moment correlation #>  #> data:  raz2005$Age and raz2005$Volume #> t = -5.165, df = 142, p-value = 7.984e-07 #> alternative hypothesis: true correlation is not equal to 0 #> 95 percent confidence interval: #>  -0.5269809 -0.2503991 #> sample estimates: #>        cor  #> -0.3976861  layout(matrix(c(1,3,4,2,3,4), 2, 3, byrow = T))  #a par(mar = c(1,4,4,2), oma = c(0,2,0,0), las = 1, cex.axis = 1.10, cex.sub = 1.10, cex.lab = 1.15) #cex.lab=1.1, cex.axis=1.1, cex.main=1.2, cex.sub=1.2) plot(Volume ~ Age, data = Time1raz2005, pch = 16,  xlab = \"\", ylab = \"\",      xlim = c(15,85), ylim = c(105,170), xaxt = \"n\") abline(a1.lm, col = \"red\", lwd = 2) axis.break(axis = 2, style = \"slash\") text(75, 170, \"Time 1\", cex = 1.5) text(18,111, adj = 0, bquote(italic('r')~\"=\"~ .(a1.lm.r))) text(18,107, adj = 0, bquote(italic('p')~.(a1.lm.p)))  title(\"A)\", adj = 0)  par(mar = c(4.5,4,1,2)) plot(Volume ~ Age, data = Time2raz2005, pch = 16, ylab = \"\",      xlim = c(15,85), ylim = c(105,170)) abline(a2.lm, col = \"red\", lwd = 2) axis.break(axis = 2, style = \"slash\") text(75, 170, \"Time 2\", cex = 1.5) text(18,111, adj = 0, bquote(italic('r')~\"=\"~ .(a2.lm.r))) text(18,107, adj = 0, bquote(italic('p')~.(a2.lm.p))) mtext(side = 2, expression(Cerebellar~Hemisphere~Volume~(cm^{3})), cex = .9,       outer = T, line = -1, las = 0)  #b par(mar = c(4.5,3,4,2)) #blueset <- brewer.pal(8, 'Blues') #pal <- colorRampPalette(blueset) pal <- colorRampPalette(kelly(n = 22))  plot(brainvolage.rmc, overall = F, palette = pal, ylab = \"\", xlab = \"Age\",       cex = 1.2, xlim = c(15,85), ylim = c(105,170))  axis.break(axis = 2, style = \"slash\") text(20,107, adj = 0, bquote(italic(r[rm])~\"=\"~ .(rmcorr.5b.r))) text(20,105, adj = 0, bquote(italic('p')~.(rmcorr.5b.p))) title(\"B)\", adj = 0)  #c plot(Volume~Age, data = avgRaz2005, ylab = \"\", xlab = \"Age\", cex = 1.2, pch = 16,       xlim = c(15,85), ylim = c(105,170)) abline(brainvolage.lm, col = \"red\", lwd = 2) axis.break(axis = 2, style = \"slash\") text(20,107, adj = 0, bquote(italic('r')~\"=\"~ .(fig.5c.r))) #incorrect positive sign in the paper text(20,105, adj = 0, bquote(italic('p')~.(fig.5c.p))) #text(20,107,paste('r =', round(c.rtest$est,2),'\\np < 0.001'), adj = 0) title(\"C)\", adj = 0) #Figure 5. Comparison of rmcorr and simple regression/correlation results for age and brain structure volume data.  #Each dot represents one of two separate observations of age and CBH for a participant. (A) #Separate simple  #regressions/correlations by time: each observation is treated as independent, represented by shading all the data  #points black. The red line is the fit of the simple regression/correlation. (B) Rmcorr: observations from the same  #participant are given the same color, with corresponding lines to show the rmcorr fit for each participant. (C)  #Simple regression/correlation: averaged by participant. Note that the effect size is greater (stronger negative  #relationship) using rmcorr (B) than with either use of simple regression models (A) and (C). This figure was  #created using data from Raz et al. (2005). #dev.copy2eps(file=\"plots/Figure5_Volume_Age.eps\", width = 9, height = 6) #dev.copy(pdf, file=\"plots/Figure5_Volume_Age.pdf\", height = 6, width = 6) #dev.off()"},{"path":[]},{"path":"/articles/New_rmcorr_paper_analyses_figures.html","id":"rmcorr-and-simple-reg-results-1","dir":"Articles","previous_headings":"6. Visual search rmcorr and simple reg/cor results and Figure 6","what":"rmcorr and simple reg results","title":"Reproduce Paper Results","text":"","code":"#a - rmcorr vissearch.rmc <- rmcorr(participant = sub, measure1 = rt, measure2 = acc, dataset = gilden2010) #> Warning in rmcorr(participant = sub, measure1 = rt, measure2 = acc, dataset = #> gilden2010): 'sub' coerced into a factor print(vissearch.rmc) #>  #> Repeated measures correlation #>  #> r #> -0.406097 #>  #> degrees of freedom #> 32 #>  #> p-value #> 0.01716871 #>  #> 95% confidence interval #> -0.6543958 -0.07874527  #b - averaged data gildenMeans <- aggregate(gilden2010[,3:4], by = list(gilden2010$sub), mean) avg.lm <- lm(acc ~ rt, data = gildenMeans) print(avg.lm) #>  #> Call: #> lm(formula = acc ~ rt, data = gildenMeans) #>  #> Coefficients: #> (Intercept)           rt   #>      0.8132       0.1777 b.rtest <- cor.test(gildenMeans$rt, gildenMeans$acc) print(b.rtest) #>  #>  Pearson's product-moment correlation #>  #> data:  gildenMeans$rt and gildenMeans$acc #> t = 2.1966, df = 9, p-value = 0.05565 #> alternative hypothesis: true correlation is not equal to 0 #> 95 percent confidence interval: #>  -0.01409542  0.87910346 #> sample estimates: #>       cor  #> 0.5907749  #c - aggregated data (overfit, incorrectly treated as independent participants/observations) agg.lm <- lm(acc ~ rt, data = gilden2010) print(agg.lm) #>  #> Call: #> lm(formula = acc ~ rt, data = gilden2010) #>  #> Coefficients: #> (Intercept)           rt   #>      0.8612       0.1111 c.rtest <- cor.test(gilden2010$rt, gilden2010$acc) print(c.rtest) #>  #>  Pearson's product-moment correlation #>  #> data:  gilden2010$rt and gilden2010$acc #> t = 2.6401, df = 42, p-value = 0.01158 #> alternative hypothesis: true correlation is not equal to 0 #> 95 percent confidence interval: #>  0.09053513 0.60625185 #> sample estimates: #>       cor  #> 0.3772751  par(mfrow=c(1,3), mar=c(5,4.6,4,0.5), mgp=c(3.2,0.8,0),  oma = c(0, 0, 0, 0), las = 1, cex.axis = 1.2, cex.sub = 1.1, cex.lab = 1.2)  #, cex.axis = 1.10, cex.sub = 1.10, cex.lab = 1.15)  plot(vissearch.rmc, overall = F, xlab = \"Response Time (seconds)\",       ylab = \"Accuracy\", cex = 1.2,      ylim = c(.79, 1), xlim = c(0.45, .95))  axis.break(axis = 1, style = \"slash\") axis.break(axis = 2, style = \"slash\")               #example of rounding and pvals.fct inside text() text(0.95,0.8, adj = 1,    bquote(italic(r[rm])~\"=\"~.(round(vissearch.rmc$r, digits = 2))), cex = 1.2) text(0.95,0.7925, adj = 1, bquote(italic('p')~\"<\"~.(pvals.fct(vissearch.rmc$p))), cex = 1.2) title(\"A)\", adj = 0)  plot(acc~rt, data = gildenMeans, cex = 1.2, pch = 16, ylim = c(.79, 1),       xlim = c(0.45, .95), xlab = \"Response Time (seconds)\",  ylab = \"\") abline(avg.lm, col = \"red\", lwd = 2) axis.break(axis = 1, style = \"slash\") axis.break(axis = 2, style = \"slash\") text(0.95,0.8,    adj = 1, bquote(italic('r')~\"=\"~ .(round(b.rtest$estimate, digits = 2))), cex = 1.2) text(0.95,0.7925, adj = 1, bquote(italic('p')~\"=\"~.(pvals.fct(b.rtest$p.value))), cex = 1.2) #text(.95,.8,paste('r =', round(b.rtest$est,2),'\\np =', round(b.rtest$p.value,2)), adj = 1) title(\"B)\", adj = 0)  plot(acc~rt, data = gilden2010, xlab = \"Response Time (seconds)\", ylab = \"\",      cex = 1.2, pch = 16, ylim = c(.79, 1), xlim = c(0.45, .95)) abline(agg.lm, col = \"red\", lwd = 2) axis.break(axis = 1, style = \"slash\") axis.break(axis = 2, style = \"slash\") text(0.95,0.8, adj = 1, bquote(italic('r')~\"=\"~ .(round(c.rtest$estimate, digits = 2))), cex = 1.2) text(0.95,0.7925, adj = 1, bquote(italic('p')~\"<\"~.(pvals.fct(c.rtest$p.value))), cex = 1.2) title(\"C)\", adj = 0) #text(.95,.8,paste('r =', round(c.rtest$est,2),'\\np =', round(c.rtest$p.value,2)), adj = 1)  #Figure 6. The x-axis is reaction time (seconds) and the y-axis is accuracy in visual search. (A) Rmcorr: each dot represents  #the average reaction time and accuracy for a block, color identifies participant, #and colored lines show rmcorr fits for  #each participant. (B) Simple regression/correlation (averaged data): each dot represents a block, (improperly) treated as an  #independent observation. The red line is #the fit to the simple regression/correlation. (C) Simple regression/correlation  #(aggregated data): improperly treating each dot as independent. This figure was created using data from Gilden et al. (2010). #dev.copy2eps(file=\"plots/Figure6_Visual_Search.eps\", width = 9, height = 6) #dev.copy(pdf, file=\"plots/Figure6_Visual_Search.pdf\", height = 9, width = 6) #dev.off()"},{"path":"/articles/New_rmcorr_paper_analyses_figures.html","id":"appendix-c","dir":"Articles","previous_headings":"","what":"Appendix C","title":"Reproduce Paper Results","text":"Rmcorr multilevel model Raz et al. 2005 data  Rmcorr multilevel model Gilden et al. 2010 data","code":"brainvolage.rmc <- rmcorr(participant = Participant, measure1 = Age, measure2 = Volume, dataset = raz2005)  #Null multilevel model: Random intercept and fixed slope null.vol <- lmer(Volume ~ Age + (1 | Participant), data = raz2005, REML = FALSE)  #Model fit null.vol #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: Volume ~ Age + (1 | Participant) #>    Data: raz2005 #>       AIC       BIC    logLik  deviance  df.resid  #>  977.4705  989.3497 -484.7352  969.4705       140  #> Random effects: #>  Groups      Name        Std.Dev. #>  Participant (Intercept) 11.287   #>  Residual                 3.024   #> Number of obs: 144, groups:  Participant, 72 #> Fixed Effects: #> (Intercept)          Age   #>    163.7090      -0.5533  #Parameter Confidence Intervals confint(null.vol) #> Computing profile confidence intervals ... #>                   2.5 %      97.5 % #> .sig01        9.5208850  13.6036847 #> .sigma        2.5713387   3.6236647 #> (Intercept) 155.1479333 172.3796868 #> Age          -0.7016833  -0.4056009  #Model fitted values and confidence intervals for each participant (L1 effects) set.seed(9999) L1.predict.raz <- predictInterval(null.vol, newdata = raz2005, n.sims = 1000) L1.predict.raz <- cbind(raz2005$Participant, L1.predict.raz)  theme_minimal =  theme_bw() +             theme(                   legend.position=\"none\",                   axis.line.x = element_line(color=\"black\", size = 0.9),                   axis.line.y = element_line(color=\"black\", size = 0.9),                   axis.text.x = element_text(size = 12),                   axis.text.y = element_text(size = 12),                    axis.title.x = element_text(size = 12),                   axis.title.y = element_text(size = 12)                   )   #Create custom color palette  # Blues<-brewer.pal(9,\"Blues\")  ggplot(raz2005, aes(x = Age, y = Volume, group = Participant, color = Participant)) +   geom_line(aes(y = predict(null.vol)), linetype = 2) +    geom_line(aes(y = brainvolage.rmc$model$fitted.values), linetype = 1) +    geom_ribbon(aes(ymin = L1.predict.raz$lwr,                   ymax = L1.predict.raz$upr,                   group = L1.predict.raz$`raz2005$Participant`,                   linetype = NA), alpha = 0.07) +   theme_minimal +    labs(title = \"Rmcorr and Random Intercept Multilevel Model:\\n Raz et al. 2005 Data\", x = \"Age\",         y = expression(Cerebellar~Hemisphere~Volume~(cm^{3}))) +   geom_point(aes(colour = Participant)) +   scale_colour_gradientn(colours=kelly(22)) + theme(plot.title = element_text(hjust = 0.5)) +    geom_abline(intercept = fixef(null.vol)[1], slope = fixef(null.vol)[2], colour = \"black\", size = 1, linetype = 2) #Appendix C, Figure 1: Dots are actual data values, with color indicating participant. Solid #colored lines show the rmcorr model fit. The multilevel model fit is indicated by the dashed #colored lines for Level 1 (participant) effects and the dashed black line for Level 2 (experiment) #effects. The shaded areas are 95% confidence intervals for Level 1 effects. Note the models #clearly overlap, despite the absence of confidence intervals for rmcorr.   #Converted to EPS file using Acrobat Pro b/c EPS doesn't support transparency #ggsave(file = \"plots/AppendixC_Figure1.pdf\", width = 5.70 , height = 5.73, dpi = 300) #dev.off() vissearch.rmc <- rmcorr(participant = sub, measure1 = rt, measure2 = acc, dataset = gilden2010)  null.vis <- lmer(acc ~ rt + (1 | sub), data = gilden2010, REML = FALSE)  #Model 1: Random intercept + random slope for RT model1.vis <- lmer(acc ~ rt + (1 + rt | sub), data = gilden2010, REML = FALSE) #> boundary (singular) fit: see help('isSingular')  #Model Comparison #a) Chi-Square anova(null.vis, model1.vis) #> Data: gilden2010 #> Models: #> null.vis: acc ~ rt + (1 | sub) #> model1.vis: acc ~ rt + (1 + rt | sub) #>            npar     AIC     BIC logLik deviance  Chisq Df Pr(>Chisq) #> null.vis      4 -197.41 -190.27 102.70  -205.41                      #> model1.vis    6 -193.46 -182.75 102.73  -205.46 0.0497  2     0.9754  #b) Evidence ratio using AIC Models.vis<-list() Models.vis<-c(null.vis, model1.vis)  if (requireNamespace(\"AICcmodavg\", quietly = TRUE)){   ModelTable2<-aictab(Models.vis, modnames = c(\"null\", \"Model 1\"))   ModelTable2   evidence(ModelTable2) } #>  #> Evidence ratio between models 'null' and 'Model 1': #> 13.43  #Estimating and graphing null model  #Parameter Confidence Intervals confint(null.vis) #> Computing profile confidence intervals ... #>                   2.5 %     97.5 % #> .sig01       0.02138120 0.05796124 #> .sigma       0.01294851 0.02139924 #> (Intercept)  0.91815424 1.05718722 #> rt          -0.15404840 0.02955915  #Model fitted values and confidence intervals for each participant (L1 effects) set.seed(9999) L1.predict.gilden <- predictInterval(null.vis, newdata = gilden2010, n.sims = 1000) L1.predict.gilden <- cbind(gilden2010$sub, L1.predict.gilden)   theme_minimal =  theme_bw() +             theme(                   legend.position=\"none\",                   axis.line.x = element_line(color=\"black\", size = 0.9),                   axis.line.y = element_line(color=\"black\", size = 0.9),                   axis.text.x = element_text(size = 12),                   axis.text.y = element_text(size = 12),                    axis.title.x = element_text(size = 12),                   axis.title.y = element_text(size = 12)                   )  #Create custom color palette  Colors12<-brewer.pal(12,\"Paired\")  ggplot(gilden2010, aes(x = rt, y = acc, group = sub, color = sub)) +   geom_line(aes(y = predict(null.vis)), linetype = 2) +    geom_line(aes(y = vissearch.rmc$model$fitted.values), linetype = 1) +    geom_ribbon(aes(ymin = L1.predict.gilden$lwr,                   ymax = L1.predict.gilden$upr,                   group = L1.predict.gilden$`gilden2010$sub`,                   linetype = NA),                   alpha = 0.07) +   theme_minimal + theme(plot.title = element_text(hjust = 0.5)) +    labs(title = \"Rmcorr and Random Intercept Multilevel Model:\\n Gilden et al. 2010 Data\", x = \"Response Time (Seconds)\", y = \"Accuracy\") +   geom_point(aes(colour = sub)) +   scale_colour_gradientn(colours=Colors12) +    geom_abline(intercept = fixef(null.vis)[1], slope = fixef(null.vis)[2], colour = \"black\", size = 1, linetype = 2) +   scale_y_continuous(breaks=seq(0.80, 1.0, 0.05)) +   scale_x_continuous(breaks=seq(0.50, 0.9, 0.1)) #Converted to EPS file using Acrobat Pro b/c EPS doesn't support transparency  #Appendix C, Figure 2: Dots are actual data values, with color indicating participant. Solid #colored lines show the rmcorr model fit. The multilevel model fit is indicated by the dashed #colored lines for Level 1 (participant) effects and the dashed black line for Level 2 (experiment) #effects. The shaded areas are 95% confidence intervals for Level 1 effects. Note the models #clearly overlap, despite the absence of confidence intervals for rmcorr.  #ggsave(file = \"plots/AppendixC_Figure2.pdf\", width = 6.5 , height = 6.5, dpi = 300)   #Estimating CIs: Convergence problems with model 1   confint(model1.vis) #> Computing profile confidence intervals ... #>                   2.5 %     97.5 % #> .sig01       0.02197527 0.11293808 #> .sig02      -1.00000000 1.00000000 #> .sig03       0.00000000        Inf #> .sigma       0.01303108 0.02157782 #> (Intercept)  0.91458725 1.06378202 #> rt          -0.15425292 0.03364490   warnings()      set.seed(9999)   predictInterval(model1.vis, newdata = gilden2010, n.sims = 1000) #>          fit       upr       lwr #> 1  0.8663920 0.8960908 0.8352832 #> 2  0.8683327 0.8992494 0.8385416 #> 3  0.8707389 0.8997923 0.8393785 #> 4  0.8725921 0.9043416 0.8411908 #> 5  0.9429413 0.9732029 0.9159473 #> 6  0.9432319 0.9722811 0.9162444 #> 7  0.9500212 0.9787765 0.9220920 #> 8  0.9521220 0.9792167 0.9237837 #> 9  0.9481541 0.9752953 0.9207015 #> 10 0.9557798 0.9844996 0.9267692 #> 11 0.9559257 0.9853144 0.9268080 #> 12 0.9596370 0.9887583 0.9285575 #> 13 0.9443758 0.9730895 0.9144351 #> 14 0.9430301 0.9705867 0.9121712 #> 15 0.9481910 0.9777425 0.9189827 #> 16 0.9500669 0.9785099 0.9180516 #> 17 0.9546490 0.9828561 0.9234057 #> 18 0.9511326 0.9826628 0.9211205 #> 19 0.9583308 0.9867742 0.9280774 #> 20 0.9601212 0.9901520 0.9312060 #> 21 0.9227967 0.9488316 0.8935664 #> 22 0.9244902 0.9553428 0.8959148 #> 23 0.9277636 0.9569939 0.8986043 #> 24 0.9300477 0.9599092 0.9000850 #> 25 0.9625122 0.9928161 0.9347764 #> 26 0.9680937 0.9978023 0.9390449 #> 27 0.9706817 1.0010774 0.9416283 #> 28 0.9749069 1.0033890 0.9446229 #> 29 0.9337347 0.9613853 0.9051130 #> 30 0.9427611 0.9688831 0.9133651 #> 31 0.9495472 0.9767039 0.9189270 #> 32 0.9506709 0.9813415 0.9226078 #> 33 0.9092402 0.9374052 0.8801574 #> 34 0.9081569 0.9380428 0.8783812 #> 35 0.9101271 0.9393227 0.8784178 #> 36 0.9130451 0.9411345 0.8840515 #> 37 0.9530667 0.9841683 0.9233226 #> 38 0.9633384 0.9921144 0.9363187 #> 39 0.9616431 0.9909585 0.9335434 #> 40 0.9655959 0.9935503 0.9362402 #> 41 0.9739775 1.0029519 0.9440092 #> 42 0.9725509 1.0019449 0.9447089 #> 43 0.9767107 1.0060772 0.9455765 #> 44 0.9756523 1.0049782 0.9452859   warnings()"},{"path":"/articles/New_rmcorr_paper_analyses_figures.html","id":"diagnostic-plot-rmcorr-and-straight-lines-between-points-not-in-paper","dir":"Articles","previous_headings":"Appendix C","what":"Diagnostic Plot: Rmcorr and straight lines between points (not in paper)","title":"Reproduce Paper Results","text":"","code":"brainvolage.rmc <- rmcorr(participant = Participant, measure1 = Age, measure2 = Volume, dataset = raz2005) #> Warning in rmcorr(participant = Participant, measure1 = Age, measure2 = Volume, #> : 'Participant' coerced into a factor print(brainvolage.rmc) #>  #> Repeated measures correlation #>  #> r #> -0.7044077 #>  #> degrees of freedom #> 71 #>  #> p-value #> 3.561007e-12 #>  #> 95% confidence interval #> -0.804153 -0.56608  theme_minimal =  theme_bw() +             theme(                   legend.position=\"none\",                   axis.line.x = element_line(color=\"black\", size = 0.9),                   axis.line.y = element_line(color=\"black\", size = 0.9),                   axis.text.x = element_text(size = 12),                   axis.text.y = element_text(size = 12),                    axis.title.x = element_text(size = 12),                   axis.title.y = element_text(size = 12)                   )  ggplot(raz2005, aes(x = Age, y = Volume, group = Participant, color = Participant)) +   geom_line(aes(y = brainvolage.rmc$model$fitted.values), linetype = 1) +    theme_minimal +    labs(title = \"Rmcorr and Diagnostic Lines:\\n Raz et al. 2005 Data\", x = \"Age\",         y = expression(Cerebellar~Hemisphere~Volume~(cm^{3}))) +   geom_point(aes(colour = Participant)) +   scale_colour_gradientn(colours=kelly(22)) + theme(plot.title = element_text(hjust = 0.5)) + geom_line(linetype = 3) #ggsave(file = \"plots/Figure_rmcorr_diagnostic.pdf\", width = 5.70 , height = 5.73, dpi = 300) #ggsave(file = \"plots/Figure_rmcorr_diagnostic.eps\", width = 5.70 , height = 5.73, dpi = 300) #dev.off()"},{"path":"/articles/compcor.html","id":"comparing-correlations","dir":"Articles","previous_headings":"","what":"Comparing Correlations","title":"Comparing Correlations","text":"show examples comparing magnitude two rrm values. Correlations compared using cocor package (Diedenhofen Musch 2015): R package Web Version","code":""},{"path":"/articles/compcor.html","id":"running-examples-requires-cocor","dir":"Articles","previous_headings":"Comparing Correlations","what":"Running Examples Requires cocor","title":"Comparing Correlations","text":"","code":"#Install cocor install.packages(\"cocor\") require(cocor)"},{"path":"/articles/compcor.html","id":"independent-correlations","dir":"Articles","previous_headings":"Comparing Correlations","what":"Independent Correlations","title":"Comparing Correlations","text":"first example, compare rrm values two distinct, independent datasets. , participants. nonsense example two datasets completely different experimental designs share common measures.  Note two rrm values similar magnitude large overlap confidence intervals: rrm = -0.58, 95% CI [-0.74, -0.38] rrm = -0.40, 95% CI [-0.66, -0.07]. Thus, significantly different.","code":"#1) Run rmcorr on two different datasets model1.marusich2016_exp2  <- rmcorr(Pair, HVT_capture, MARS, marusich2016_exp2) #> Warning in rmcorr(Pair, HVT_capture, MARS, marusich2016_exp2): 'Pair' coerced #> into a factor model1.marusich2016_exp2 #>  #> Repeated measures correlation #>  #> r #> -0.5890471 #>  #> degrees of freedom #> 55 #>  #> p-value #> 1.434929e-06 #>  #> 95% confidence interval #> -0.7365623 -0.3880381  model2.gilden2010         <- rmcorr(sub, rt, acc, gilden2010 ) #> Warning in rmcorr(sub, rt, acc, gilden2010): 'sub' coerced into a factor model2.gilden2010 #>  #> Repeated measures correlation #>  #> r #> -0.406097 #>  #> degrees of freedom #> 32 #>  #> p-value #> 0.01716871 #>  #> 95% confidence interval #> -0.6543958 -0.07874527  #2) Extract relevant parameters #Model 1 rmcorr1 <- model1.marusich2016_exp2$r rmcorr1 #> [1] -0.5890471  n1 <- model1.marusich2016_exp2$df + 2 #note the same kludge as power above n1                                    #this is the effective sample size #> [1] 57  #Model 2 rmcorr2 <- model2.gilden2010$r rmcorr2 #> [1] -0.406097  n2 <- model2.gilden2010$df + 2  n2 #> [1] 34  #3) Compare the two indendent rmcorr coefficients cocor.indep.groups(rmcorr1, rmcorr2, n1, n2,                     var.labels = c(model1.marusich2016_exp2$var[2:3],                                   model2.gilden2010$vars[2:3])) #>  #>   Results of a comparison of two correlations based on independent groups #>  #> Comparison between r1.jk (HVT_capture, MARS) = -0.589 and r2.hm (rt, acc) = -0.4061 #> Difference: r1.jk - r2.hm = -0.183 #> Data: j = HVT_capture, k = MARS, h = rt, m = acc #> Group sizes: n1 = 57, n2 = 34 #> Null hypothesis: r1.jk is equal to r2.hm #> Alternative hypothesis: r1.jk is not equal to r2.hm (two-sided) #> Alpha: 0.05 #>  #> fisher1925: Fisher's z (1925) #>   z = -1.0885, p-value = 0.2764 #>   Null hypothesis retained #>  #> zou2007: Zou's (2007) confidence interval #>   95% confidence interval for r1.jk - r2.hm: -0.5420 0.1365 #>   Null hypothesis retained (Interval includes 0)"},{"path":"/articles/compcor.html","id":"correlated-correlations","dir":"Articles","previous_headings":"Comparing Correlations","what":"Correlated Correlations","title":"Comparing Correlations","text":"next two examples, compare rrm dataset. dependent groups – data participants.","code":""},{"path":"/articles/compcor.html","id":"overlapping-correlations","dir":"Articles","previous_headings":"Comparing Correlations > Correlated Correlations","what":"Overlapping Correlations","title":"Comparing Correlations","text":"overlapping correlations, common variable. , compare correlations action measures distance perception. first correlation Blindwalk Away Blindwalk Toward (rrm = 0.81, 95% CI [0.75, 0.85]) second Blindwalk Toward Triangulated Blindwalk (rrm = 0.23, 95% CI [0.08, 0.36]). Note correlations significantly different 0 substantially different magnitudes wide separation confidence intervals. Hence, two correlations significantly different p < 0.0001 9 comparison tests. Also, note overlapping common variable Blindwalk Toward.","code":"variables.overlap<- c(\"Blindwalk Away\",                       \"Blindwalk Toward\",                       \"Triangulated BW\")  dist_rmc_mat_overlap <- rmcorr_mat(participant = Subject,                                     variables = variables.overlap,                                     dataset = twedt_dist_measures,                                    CI.level = 0.95)  #dist_rmc_mat_action$summary  #Use summary component  model1.bwa.bwt <- dist_rmc_mat_overlap$summary[1,]  model2.bwa.tri <- dist_rmc_mat_overlap$summary[2,] model3.bwt.tri <- dist_rmc_mat_overlap$summary[3,]  r.jk <- model1.bwa.bwt$rmcorr.r r.jh <- model2.bwa.tri$rmcorr.r #overlap r.kh <- model3.bwt.tri$rmcorr.r   #Since there is missing data, the results are unbalanced. We use the average effective sample size. n    <- mean(dist_rmc_mat_overlap$summary$effective.N)  cocor.dep.groups.overlap(r.jk,                           r.jh,                           r.kh,                           n,                           alternative = \"two.sided\",                           test = \"all\",                           var.labels = variables.overlap) #Same as variables used in rmcorr_mat() #>  #>   Results of a comparison of two overlapping correlations based on dependent groups #>  #> Comparison between r.jk (Blindwalk Away, Blindwalk Toward) = 0.8066 and r.jh (Blindwalk Away, Triangulated BW) = 0.2383 #> Difference: r.jk - r.jh = 0.5683 #> Related correlation: r.kh = 0.2255 #> Data: j = Blindwalk Away, k = Blindwalk Toward, h = Triangulated BW #> Group size: n = 177 #> Null hypothesis: r.jk is equal to r.jh #> Alternative hypothesis: r.jk is not equal to r.jh (two-sided) #> Alpha: 0.05 #>  #> pearson1898: Pearson and Filon's z (1898) #>   z = 7.8559, p-value = 0.0000 #>   Null hypothesis rejected #>  #> hotelling1940: Hotelling's t (1940) #>   t = 10.2385, df = 174, p-value = 0.0000 #>   Null hypothesis rejected #>  #> williams1959: Williams' t (1959) #>   t = 9.3823, df = 174, p-value = 0.0000 #>   Null hypothesis rejected #>  #> olkin1967: Olkin's z (1967) #>   z = 7.8559, p-value = 0.0000 #>   Null hypothesis rejected #>  #> dunn1969: Dunn and Clark's z (1969) #>   z = 8.7407, p-value = 0.0000 #>   Null hypothesis rejected #>  #> hendrickson1970: Hendrickson, Stanley, and Hills' (1970) modification of Williams' t (1959) #>   t = 10.2368, df = 174, p-value = 0.0000 #>   Null hypothesis rejected #>  #> steiger1980: Steiger's (1980) modification of Dunn and Clark's z (1969) using average correlations #>   z = 8.5460, p-value = 0.0000 #>   Null hypothesis rejected #>  #> meng1992: Meng, Rosenthal, and Rubin's z (1992) #>   z = 8.3907, p-value = 0.0000 #>   Null hypothesis rejected #>   95% confidence interval for r.jk - r.jh: 0.6700 1.0784 #>   Null hypothesis rejected (Interval does not include 0) #>  #> hittner2003: Hittner, May, and Silver's (2003) modification of Dunn and Clark's z (1969) using a backtransformed average Fisher's (1921) Z procedure #>   z = 8.3966, p-value = 0.0000 #>   Null hypothesis rejected #>  #> zou2007: Zou's (2007) confidence interval #>   95% confidence interval for r.jk - r.jh: 0.4288 0.7139 #>   Null hypothesis rejected (Interval does not include 0)"},{"path":"/articles/compcor.html","id":"non-overlapping-correlations","dir":"Articles","previous_headings":"Comparing Correlations > Correlated Correlations","what":"Non-Overlapping Correlations","title":"Comparing Correlations","text":"non-overlapping correlations, data participants overlapping variable comparison. compare correlations two action measures distance perception (blindwalk away blindwalk toward) two direct measures (verbal visual matching). respective correlations (rrm = 0.81, 95% CI [0.75, 0.85]) (rrm = 0.73, 95% CI [0.66, 0.80]). non-overlapping correlations similar magnitude partially overlapping confidence intervals. Thus, barely significantly different p < 0.04.","code":"variables.nonoverlap  <- c(\"Blindwalk Away\",                            \"Blindwalk Toward\",                            \"Verbal\",                            \"Visual matching\")  dist_rmc_mat_nonoverlap <- rmcorr_mat(participant = Subject,                                        variables = variables.nonoverlap,                                        dataset = twedt_dist_measures,                                       CI.level = 0.95)  dist_rmc_mat_nonoverlap$summary #>           measure1         measure2  df  rmcorr.r   lowerCI   upperCI #> 1   Blindwalk Away Blindwalk Toward 175 0.8065821 0.7480818 0.8526427 #> 2   Blindwalk Away           Verbal 175 0.7355813 0.6596521 0.7966468 #> 3   Blindwalk Away  Visual matching 174 0.7758245 0.7093042 0.8286489 #> 4 Blindwalk Toward           Verbal 177 0.7160551 0.6362000 0.7807308 #> 5 Blindwalk Toward  Visual matching 177 0.7575109 0.6871894 0.8137687 #> 6           Verbal  Visual matching 179 0.7341831 0.6588827 0.7949162 #>         p.vals effective.N #> 1 8.228992e-42         177 #> 2 2.056415e-31         177 #> 3 1.226384e-36         176 #> 4 1.937983e-29         179 #> 5 1.302874e-34         179 #> 6 6.400493e-32         181  #Use summary component  model1.bwa.bwt   <- dist_rmc_mat_nonoverlap$summary[1,]  model2.verb.vis  <- dist_rmc_mat_nonoverlap$summary[6,] model3.bwa.verb  <- dist_rmc_mat_nonoverlap$summary[2,] model4.bwa.vis   <- dist_rmc_mat_nonoverlap$summary[3,]  model5.bwt.verb  <- dist_rmc_mat_nonoverlap$summary[4,]  model6.bwt.vis   <- dist_rmc_mat_nonoverlap$summary[5,]   #Cheatsheet     #j = bwa     #k = bwt     #h = verb     #m = vis  r.jk <- model1.bwa.bwt$rmcorr.r  #Action measures r.hm <- model2.verb.vis$rmcorr.r #Direct measures r.jh <- model3.bwa.verb$rmcorr.r #bwa ~ verb r.jm <- model4.bwa.vis$rmcorr.r  #bwa ~ vis r.kh <- model5.bwt.verb$rmcorr.r #bwt ~ verb r.km <- model6.bwt.vis$rmcorr.r  #bwt ~ vis  #Since there is missing data, we use the average effective sample size. n    <- round(mean(dist_rmc_mat_nonoverlap$summary$effective.N), digits = 0) + 2  cocor.dep.groups.nonoverlap(r.jk,                             r.hm,                              r.jh,                              r.jm,                              r.kh,                              r.km,                              n,                              alternative = \"two.sided\",                              test = \"all\",                              var.labels = variables.nonoverlap) #Same as variables used in rmcorr_mat() #>  #>   Results of a comparison of two nonoverlapping correlations based on dependent groups #>  #> Comparison between r.jk (Blindwalk Away, Blindwalk Toward) = 0.8066 and r.hm (Verbal, Visual matching) = 0.7342 #> Difference: r.jk - r.hm = 0.0724 #> Related correlations: r.jh = 0.7356, r.jm = 0.7758, r.kh = 0.7161, r.km = 0.7575 #> Data: j = Blindwalk Away, k = Blindwalk Toward, h = Verbal, m = Visual matching #> Group size: n = 180 #> Null hypothesis: r.jk is equal to r.hm #> Alternative hypothesis: r.jk is not equal to r.hm (two-sided) #> Alpha: 0.05 #>  #> pearson1898: Pearson and Filon's z (1898) #>   z = 2.0671, p-value = 0.0387 #>   Null hypothesis rejected #>  #> dunn1969: Dunn and Clark's z (1969) #>   z = 2.0989, p-value = 0.0358 #>   Null hypothesis rejected #>  #> steiger1980: Steiger's (1980) modification of Dunn and Clark's z (1969) using average correlations #>   z = 2.0983, p-value = 0.0359 #>   Null hypothesis rejected #>  #> raghunathan1996: Raghunathan, Rosenthal, and Rubin's (1996) modification of Pearson and Filon's z (1898) #>   z = 2.0989, p-value = 0.0358 #>   Null hypothesis rejected #>  #> silver2004: Silver, Hittner, and May's (2004) modification of Dunn and Clark's z (1969) using a backtransformed average Fisher's (1921) Z procedure #>   z = 2.0966, p-value = 0.0360 #>   Null hypothesis rejected #>  #> zou2007: Zou's (2007) confidence interval #>   95% confidence interval for r.jk - r.hm: 0.0048 0.1456 #>   Null hypothesis rejected (Interval does not include 0)"},{"path":"/articles/estimates_w_NaN.html","id":"nan-estimates","dir":"Articles","previous_headings":"","what":"NaN estimates","title":"rmcorr Estimates with NaN","text":"synthetic dataset produces NaN estimates rmcorr. Thanks Shreya Ghosh example.","code":""},{"path":"/articles/estimates_w_NaN.html","id":"running-examples-requires-ggextra-ggextra","dir":"Articles","previous_headings":"NaN estimates","what":"Running Examples Requires ggExtra (Attali and Baker 2022)","title":"rmcorr Estimates with NaN","text":"","code":"install.packages(\"ggExtra\") require(ggExtra)"},{"path":"/articles/estimates_w_NaN.html","id":"load-data-visualize-and-model","dir":"Articles","previous_headings":"NaN estimates","what":"Load data, visualize, and model","title":"rmcorr Estimates with NaN","text":"NaN estimates appear due insufficient varability dataset. possible way address issue adding small amount random noise.","code":"load(file = \"../man/data/ghosh_synth.rda\")  #Look at data ghosh_synth #Note lots of repeated zeros in A3 and A4 #>    Subject TP  A1 A3 A4 A5  A6 #> 1        1  1   0  0  0 11   0 #> 2        1  2   0  0  0  3   0 #> 3        2  1   0  5  0  2   0 #> 4        2  2   0  0  0 16   0 #> 5        3  1  72  0 11  0   0 #> 6        3  2 161  0 25  0   0 #> 7        4  1  54  9  0 10   0 #> 8        4  2  30  3  0  2   0 #> 9        5  1   0 10  6  0  33 #> 10       5  2   0 13 11  0 106 #> 11       6  1   0  0  0  0   0 #> 12       6  2   0  0  0  0   0 #> 13       7  1   0 43  0  8   0 #> 14       7  2   0 38  0 18   0 #> 15       8  1   8  8  0  0   0 #> 16       8  2   0  6  0  0  45 #> 17       9  1   0 38  0  0  48 #> 18       9  2   0 11  0  0  99 #> 19      10  1  28 22  5  0   0 #> 20      10  2   0  7  6  0 151  set.seed(40) #Make jittering reproducible  p <- ggplot(ghosh_synth, aes(x = A4, y = A3)) +             geom_point(alpha = 0.2) +             geom_jitter(width = 2, height = 2)  p1 <- ggMarginal(p, type=\"histogram\") p1 rmc.ghosh <- rmcorr(Subject, A3, A4, ghosh_synth) #> Warning in rmcorr(Subject, A3, A4, ghosh_synth): 'Subject' coerced into a #> factor  rmc.ghosh #>  #> Repeated measures correlation #>  #> r #> 0 #>  #> degrees of freedom #> 9 #>  #> p-value #> 1 #>  #> 95% confidence interval #> -0.599875 0.599875  #The default rmcorr plot doesn't jitter values, this masks identical values because they are drawn on top of each other plot(rmc.ghosh)"},{"path":"/articles/estimates_w_NaN.html","id":"add-random-noise","dir":"Articles","previous_headings":"NaN estimates","what":"Add random noise","title":"rmcorr Estimates with NaN","text":"","code":"set.seed(67)  small.noise1 <- rnorm(dim(ghosh_synth)[[1]], 0, 0.2) small.noise2 <- rnorm(dim(ghosh_synth)[[1]], 0, 0.2)      ghosh_synth$A3.noise <- ghosh_synth$A3 + small.noise1 ghosh_synth$A4.noise <- ghosh_synth$A4 + small.noise2  rmc.ghosh.noise <- rmcorr(Subject, A3.noise, A4.noise, ghosh_synth) #> Warning in rmcorr(Subject, A3.noise, A4.noise, ghosh_synth): 'Subject' coerced #> into a factor  rmc.ghosh.noise #>  #> Repeated measures correlation #>  #> r #> -0.02006963 #>  #> degrees of freedom #> 9 #>  #> p-value #> 0.9532957 #>  #> 95% confidence interval #> -0.6125697 0.5868709  p2 <- ggplot(ghosh_synth, aes(x = A3.noise, y = A4.noise,         group = factor(Subject), color = factor(Subject))) +        ggplot2::geom_point(ggplot2::aes(colour = factor(Subject),                                          alpha = 0.10)) +        ggplot2::geom_line(aes(y = rmc.ghosh.noise$model$fitted.values),                          linetype = 1) +      theme(legend.position=\"none\")  p3 <- ggMarginal(p2, type=\"histogram\") p3"},{"path":"/articles/estimates_w_NaN.html","id":"caveats","dir":"Articles","previous_headings":"NaN estimates","what":"Caveats","title":"rmcorr Estimates with NaN","text":"results rmcorr interpreted caution data non-normal zero-inflation. Still, results provides least starting point: common linear association around 0. much complicated alternative fitting multilevel model appropriate distribution zero-inflated data (e.g., negative binomial distribution zero-inflated Poisson).","code":""},{"path":"/articles/model_diag.html","id":"running-example-requires-gglm-gglm","dir":"Articles","previous_headings":"","what":"Running Example Requires gglm (White 2023)","title":"Diagnostic Plots","text":"","code":"#Install gglm install.packages(\"gglm\") require(gglm)"},{"path":"/articles/model_diag.html","id":"plotting-model-diagnostics","dir":"Articles","previous_headings":"","what":"Plotting Model Diagnostics","title":"Diagnostic Plots","text":"code demonstrates plot model diagnostics rmcorr. four diagnostic plots assessing:  1. Residuals vs. Fitted values: Linearity  2. Quantile-Quantile (Q-Q): Normality residuals  3. Scale-Location: Equality variance (homoscedasticity)  4. Residuals vs. Leverage: Influential observations  much violations assumptions matter? depends. General Linear Model (GLM) typically robust deviations assumptions, severe violations may produce misleading results (Gelman, Hill, Vehtari 2020). Also, reason(s) violations can matter: “Violations assumptions may result problems dataset, use incorrect regression model, ” (Cohen et al. 2013, 117).","code":"raz.rmc <- rmcorr(participant = Participant, measure1 = Age,                    measure2 = Volume, dataset = raz2005)  #> Warning in rmcorr(participant = Participant, measure1 = Age, measure2 = Volume, #> : 'Participant' coerced into a factor  #Using gglm  gglm(raz.rmc$model) #using base R  #plot(raz.rmc$model)"},{"path":"/articles/overfitting.html","id":"overfittingpseudoreplication","dir":"Articles","previous_headings":"","what":"Overfitting/Pseudoreplication","title":"Overfitting/Pseudoreplication","text":"vignette, demonstrate potential consequences statistical analysis ignores dependencies data. overfitting may produce misestimated effect sizes suppressed variability leading underestimated p-values. phenomenon also widely referred pseudoreplication (Hurlbert 1984; Lazic 2010; Eisner 2021) “unit analysis error” (Hurlbert 2009). accessible introduction topic see Reinhart (2015) simplified illustration consequences overfitting/pseudoreplication, comprehensive. Overfitting limited simple regression/correlation multiple types pseudoreplication including data collected space time (Hurlbert 1984, 2009).","code":""},{"path":"/articles/overfitting.html","id":"running-code-below-requires-dplyr-esc-and-patchwork","dir":"Articles","previous_headings":"Overfitting/Pseudoreplication","what":"Running Code Below Requires: dplyr, esc, and patchwork","title":"Overfitting/Pseudoreplication","text":"","code":"install.packages(\"dplyr\") require(dplyr)  install.packages(\"esc\") require(esc)  install.packages(\"patchwork\") require(patchwork)"},{"path":"/articles/overfitting.html","id":"example-of-overfitting","dir":"Articles","previous_headings":"Overfitting/Pseudoreplication","what":"Example of Overfitting","title":"Overfitting/Pseudoreplication","text":"illustrate consequences overfitting/pseudoreplication using data Marusich et al. (2016). dataset sample size N = 28 dyads (two participants working together), dyad k = 3 repeated measures two variables: total 84 paired observations (N x k). two measured variables scores Mission Awareness Rating Scale (MARS) target-capture time, assessed three separate experimental blocks. Repeated measures correlation technique assessing within-subject (example, within-dyad) relationship two variables; example, dyad tend demonstrate higher awareness scores blocks also generates faster target-capture times? Sometimes, however, relevant research question -subject (dyad) associations; example, dyads tend perform better one variable also tend perform better ? answer second question, may tempting treat repeated measures data independent. demonstrate potential consequences approach. left figure, fit simple regression/correlation improperly treating 84 total observations independent units analysis. right figure, demonstrate one approach analyzing (dyad) relationship without overfitting: averaging data unit analysis (dyad). Note approach removes within (dyad) information. x-axis depicts scores Mission Awareness Rating Scale (MARS), higher values representing better situation awareness. y-axis target-capture times, time seconds capture High Value Targets (HVT) task. Smaller values (faster times) represent better task performance. band around regression line 95% confidence interval.  inferential statistics plots : Overfit (left): r(82) = -0.35, 95% CI [-0.53, -0.15], p = 0.001 (Exact p-value = 0.0010208) Average (right): r(26) = -0.09, 95% CI [-0.44, 0.30], p = 0.67 (Exact p-value = 0.6663228) overfit results, note Pearson correlation 82 degrees freedom excessive implies sample size N = 84 dyads (N - 2 degrees freedom correlation (Cohen et al. 2013)). , actual sample size (N = 28 dyads) erroneously modeled 84 independent units ignoring three paired, repeated measures per dyad. two analyses produce varied results. overfit model erroneously produces precise moderate, negative correlation higher MARS values associated lower times capture targets (better performance). contrast, average model indicates weak negative correlation high uncertainty dyads.","code":"overfit.plot <-      ggplot(data = marusich2016_exp2, aes(x = MARS, y = HVT_capture)) +     geom_point(aes(colour = factor(Pair))) +     geom_smooth(method= \"lm\", level = 0.95) +     coord_cartesian(xlim = c(2,4), ylim=c(0,30)) +      ylab(\"Capture Time (seconds)\") +      theme_minimal() +     theme(legend.position=\"none\")   marusich2016_avg <- marusich2016_exp2 %>%                     group_by(Pair) %>%                     summarize(Mean_MARS = mean(MARS),                               Mean_HVT_capture = mean(HVT_capture))  average.plot <-      ggplot(data = marusich2016_avg,             aes(x = Mean_MARS, y = Mean_HVT_capture)) +     geom_smooth(fullrange = TRUE, method= \"lm\", level = 0.95) +     coord_cartesian(xlim = c(2,4), ylim=c(0,30)) +     geom_point(aes(colour = factor(Pair))) +     xlab(\"Mean MARS\") +     ylab(\"Mean Capture Time (seconds)\") +     scale_colour_discrete(name = \"Dyad\") +     theme_minimal()  overfit.cor <- cor.test(marusich2016_exp2$MARS, marusich2016_exp2$HVT_capture)  average.cor <- cor.test(marusich2016_avg$Mean_MARS, marusich2016_avg$Mean_HVT_capture)  df.s <- rbind(overfit.cor$parameter, average.cor$parameter)  r.s  <- rbind(round(rbind(overfit.cor$estimate, average.cor$estimate), digits = 2))   CI.s <- formatC(rbind(overfit.cor$conf.int,            average.cor$conf.int), digits = 2,           format = 'f')  p.vals <- rbind(round(overfit.cor$p.value, digits = 3),                  prettyNum(average.cor$p.value, digits = 2,                            drop0trailing = TRUE))  overfit.plot + average.plot  #> `geom_smooth()` using formula = 'y ~ x' #> `geom_smooth()` using formula = 'y ~ x'"},{"path":"/articles/overfitting.html","id":"comparison-to-rmcorr","dir":"Articles","previous_headings":"Overfitting/Pseudoreplication > Example of Overfitting","what":"Comparison to rmcorr","title":"Overfitting/Pseudoreplication","text":"comparison, show results conducting repeated measures correlation analysis dataset. described , method addresses different question nature within-dyad relationship, rather dyads. analysis, averaging required, dependencies data taken account.  inferential statistics rmcorr analysis : rrmcorr(55) = -0.59, 95% CI [-0.74, -0.39], p = 0.0000014.","code":"marusich.rmc <- rmcorr(participant = Pair,                         measure1 = MARS,                         measure2 = HVT_capture,                         data = marusich2016_exp2) #> Warning in rmcorr(participant = Pair, measure1 = MARS, measure2 = HVT_capture, #> : 'Pair' coerced into a factor rmc.plot <-      ggplot(data = marusich2016_exp2,             aes(x = MARS, y = HVT_capture, group = factor(Pair), color = factor(Pair))) +     geom_point(aes(colour = factor(Pair))) +     geom_line(aes(y = marusich.rmc$model$fitted.values)) +     theme_minimal() +     scale_colour_discrete(name = \"Dyad\") +     ylab(\"Capture Time (seconds)\")  rmc.r <- round(marusich.rmc$r, 2) rmc.CI <- round(marusich.rmc$CI, 2) rmc.p <- formatC(marusich.rmc$p, format = \"fg\", digits = 2)                 # prettyNum(average.cor$p.value, digits = 2,                           # drop0trailing = TRUE))  rmc.plot"},{"path":"/articles/overfitting.html","id":"levels-of-analysis","dir":"Articles","previous_headings":"","what":"Overfitting/Pseudoreplication","title":"Overfitting/Pseudoreplication","text":"analysis indicates strong within-dyad relative relationship two measures. within level analysis, higher values MARS large large relative (within-dyad) association better performance, lower target capture times; vice-versa. However, dyad--dyad comparisons MARS performance values weakly predictive level analysis. words, using specific MARS (performance) value predict quite limited dyads. Results level analysis may necessarily generalize within level analysis (Curran Bauer 2011; Fisher, Medaglia, Jeronimus 2018; Molenaar Campbell 2009)","code":""},{"path":"/articles/overfitting.html","id":"consequences-of-overfitting","dir":"Articles","previous_headings":"Overfitting/Pseudoreplication > Example of Overfitting","what":"Consequences of Overfitting:","title":"Overfitting/Pseudoreplication","text":"Misestimated effect size: overfit analysis approximately point-estimated medium negative correlation, whereas averaged analysis slightly less small negative point-estimated correlation. Spurious precision: coverage confidence intervals (visually plot matching inferential statistics) narrower overfit analysis compared averaged data. Suppressed p-value: overfitting produces medium negative effect size underestimates variance, overfit example highly statistically significant.","code":""},{"path":"/articles/overfitting.html","id":"spurious-precision-underestimation-of-variability","dir":"Articles","previous_headings":"Overfitting/Pseudoreplication","what":"Spurious Precision: Underestimation of Variability","title":"Overfitting/Pseudoreplication","text":"compare suppression variability overfit model versus correct model, use Fisher Z approximation standard deviation (sd) : 1N−3 \\frac{1}{\\sqrt{N-3}}  formula calculated using respective sample sizes model N = 84 (overfit) N = 28 (correct).  graph, red dot indicates overfit model black dot shows correct model averaged data. Note magnitude sd underestimation pseudoreplication depicted dashed vertical red line: Fisher Z sd = 0.11 overfit model half value correct model Fisher Z sd = 0.20.","code":"N.vals <- seq(5, 100, by = 1) sd.Z <- 1/sqrt(N.vals-3)  sd.overfit <- data.frame(cbind(N.vals, sd.Z))  sd.cor <- sd.Z[N.Marusich2016-4] #Have to subtract 4 because N.vals starts at 5, not 1 sd.overf <- sd.Z[total.obs.Marusich2016-4]  #We could determine the inflated sample size for the overfit model using its degrees of freedom + 2 df.s + 2  == total.obs.Marusich2016 #>         df #> [1,]  TRUE #> [2,] FALSE  sd.overfit.plot <-      ggplot(data = sd.overfit,             aes(x = N.vals, y = sd.Z)) +     coord_cartesian(xlim = c(0,100), ylim=c(0, 0.75)) +     geom_point(alpha = 0.25) +     geom_segment(x = 28, y =  sd.cor, xend = 84,                   yend =  sd.cor, linetype = 2) +     geom_segment(x = 84, y =  sd.cor, xend = 84,                   yend =  sd.overf, colour = \"red\",                  linetype = 2) +     xlab(\"Analysis Sample Size\") +      ylab(expression(paste(\"Fisher \", italic(\"Z\"), \" standard deviation\"))) +     annotate(geom = \"point\", x = 28, y = sd.cor,               colour = \"black\", size = 2) +     annotate(geom = \"point\", x = 84, y = sd.overf,               colour = \"red\", size = 2) +     theme_minimal()  sd.overfit.plot"},{"path":"/articles/overfitting.html","id":"detecting-overfitting","dir":"Articles","previous_headings":"Overfitting/Pseudoreplication","what":"Detecting Overfitting","title":"Overfitting/Pseudoreplication","text":"easiest way detect overfitting comparing sample size (degrees freedom) statistical results actual sample size, shown . However, degrees freedom reported results unavailable may still possible detect overfitting. example, actual sample size N reported (ideally exact) p-value can used calculate corresponding expected effect size. , expected effect reported effect can compared. demonstrate using esc package (Lüdecke 2019) determine expected correlation (Fisher Z) actual sample size N = 28 reported p-value (exact) = 0.0010208 overfit analysis. perform Fisher Z--r transformation, using psych package (William Revelle 2024), compare values expected correlation versus reported correlation. See last code chunk R Markdown document examples detecting overfit correlations results reported published papers (https://osf.io/cnfjt) systematic review meta-analysis (Bakdash et al. 2022).","code":"#Calculate the expected correlation using the reported p-value and actual sample size  #esc_t assumes p-value is two-tailed and uses a Fisher Z transformation calc.z <- esc_t(p = as.numeric(p.vals[1]),                        totaln = N.Marusich2016,                        es.type = \"r\")$es   reported.r = as.numeric(r.s[1])  #Overfit? fisherz2r(calc.z) != abs(reported.r)   #> [1] TRUE #Note calc.z will always be positive using esc_t() this way #If the reported.r is not a Fisher Z value - either it should be transformed or the calc.z should be transformed  fisherz2r(calc.z)  #> [1] 0.5284473 reported.r #> [1] -0.35"},{"path":"/articles/overfitting.html","id":"caveats","dir":"Articles","previous_headings":"Overfitting/Pseudoreplication > Detecting Overfitting","what":"Caveats","title":"Overfitting/Pseudoreplication","text":"Detection overfitting used caution mismatches reported versus expected values due missing excluded data reported statistics. Missing data can produce lower sample size (based reported statistics) actual sample size; opposed inflated sample size true pseudoreplication. Additionally, overfit models can adjusted penalized account dependencies data. Examples techniques include cluster-robust errors, regularization, ensemble models. Last, contend may circumstances overfitting acceptable (e.g., exploratory data analysis including visualization, separate models participant large number repeated trials [psychophysics], meaningful difference overfit model without adjustment techniques).","code":""},{"path":"/articles/overfitting.html","id":"prevelance-of-pseudoreplication","dir":"Articles","previous_headings":"Overfitting/Pseudoreplication","what":"Prevelance of Pseudoreplication","title":"Overfitting/Pseudoreplication","text":"prevalence overfitting/pseudoreplication published papers suggest frequent statistical error across disciplines. results reported single issue journal Nature Neuroscience 36% papers one results suspected overfitting 12% papers definitive evidence pseudoreplication (Lazic 2010). human factors, using results meeting inclusion systematic review, overfitting found 28% papers: 29 1031(Bakdash et al. 2022).","code":""},{"path":"/articles/repro_bootstrapping.html","id":"reproducible-example-of-bootstrapping","dir":"Articles","previous_headings":"","what":"Reproducible Example of Bootstrapping","title":"Bootstrapping Example","text":"illustrate reproducible example bootstrapping rmcorr. Note, estimated confidence interval changes bootstrapping. also show extract sampling distribution bootstrapped rmcorr effect size.  - set.seed() used make results reproducible.  - nreps set 100 run example quickly. Ideally, > 500.","code":""},{"path":"/articles/repro_bootstrapping.html","id":"rmcorr-effect-size-with-bootstrapped-95-confidence-inverval","dir":"Articles","previous_headings":"Reproducible Example of Bootstrapping","what":"Rmcorr effect size with Bootstrapped 95% Confidence Inverval","title":"Bootstrapping Example","text":"","code":"set.seed(532)  boot.blandrmc <- rmcorr(Subject, PaCO2, pH, bland1995,                         CIs = \"bootstrap\",                         nreps = 100,                          bstrap.out = T) #> Warning in rmcorr(Subject, PaCO2, pH, bland1995, CIs = \"bootstrap\", nreps = #> 100, : 'Subject' coerced into a factor boot.blandrmc #>  #> Repeated measures correlation #>  #> r #> -0.5067697 #>  #> degrees of freedom #> 38 #>  #> p-value #> 0.0008471081 #>  #> 95% confidence interval #> -0.726023 -0.05893805"},{"path":"/articles/repro_bootstrapping.html","id":"sampling-distribution-of-bootstrapped-effects","dir":"Articles","previous_headings":"Reproducible Example of Bootstrapping","what":"Sampling Distribution of Bootstrapped Effects","title":"Bootstrapping Example","text":"graph, x-axis bootstrapped rmcorr effect sizes y-axis frequency. red line mean sampling distribution blue line median sampling distribution. two values calculated bootstrap sampling distribution, note slightly differ non-bootstrapped point estimated effect size.","code":"boot.rmcorr.samplingdist <- round(boot.blandrmc$resamples, digits = 2) boot.rmcorr.mean <- mean(boot.blandrmc$resamples) boot.rmcorr.median <- median(boot.blandrmc$resamples)  x.vals <- sprintf(\"%.2f\", seq(-0.80, 0.00, by = 0.10))  hist(boot.rmcorr.samplingdist,       main = \"Sampling Distribution of Bootstrapped Effect Sizes\",      xaxt = \"n\",      xlab = \"Effect Size\",      las = 1) abline(v = boot.rmcorr.mean,   col = \"red\",  lwd = 2) abline(v = boot.rmcorr.median, col = \"blue\", lwd = 2) axis(1,       at = as.numeric(x.vals),      labels = x.vals)   #Compare point-est effect for bootstrap vs. non-bootstrap model #Boostrapped effect sizes #Mean boot.rmcorr.mean #> [1] -0.5073347 #Median boot.rmcorr.median #> [1] -0.5304375  #Non-bootstrapped  blandrmc <- rmcorr(Subject, PaCO2, pH, bland1995) #> Warning in rmcorr(Subject, PaCO2, pH, bland1995): 'Subject' coerced into a #> factor blandrmc$r #> [1] -0.5067697"},{"path":"/articles/rmcorr_mat.html","id":"running-examples-requires-corrplot-corrplot2021","dir":"Articles","previous_headings":"","what":"Running Examples Requires corrplot (Wei and Simko 2021)","title":"Correlation Matrix using rmcorr_mat","text":"","code":"#Install corrplot  install.packages(\"corrplot\")  require(corrplot)"},{"path":"/articles/rmcorr_mat.html","id":"plotting-a-correlation-matrix","dir":"Articles","previous_headings":"","what":"Plotting a Correlation Matrix","title":"Correlation Matrix using rmcorr_mat","text":"output rmcorr_mat can used used plot correlation matrix.","code":"dist_rmc_mat <- rmcorr_mat(participant = Subject,                             variables = c(\"Blindwalk Away\",                                          \"Blindwalk Toward\",                                          \"Triangulated BW\",                                          \"Verbal\",                                          \"Visual matching\"),                            dataset = twedt_dist_measures,                            CI.level = 0.95)  corrplot(dist_rmc_mat$matrix)"},{"path":"/articles/rmcorr_mat.html","id":"plotting-multiple-models","dir":"Articles","previous_headings":"","what":"Plotting Multiple Models","title":"Correlation Matrix using rmcorr_mat","text":"output can also used plot multiple models side--side.","code":"#Number of models being plotted n.models <- length(dist_rmc_mat$models)  #Change graphing parameters to plot side-by-side #with narrower margins par(mfrow = c(3,4),      mar = c(2.75, 2.4, 2.4, 1.4))  for (i in 1:n.models) {     plot(dist_rmc_mat$models[[i]])     }  #Reset graphing parameters #dev.off()"},{"path":"/articles/rmcorr_mat.html","id":"adjusting-for-multiple-comparisons","dir":"Articles","previous_headings":"","what":"Adjusting for Multiple Comparisons","title":"Correlation Matrix using rmcorr_mat","text":"third component output rmcorr_mat() contains summary results. Using summary component, demonstrate adjusting multiple comparisons using two methods: Bonferroni correction False Discovery Rate (FDR).  example also compares unadjusted p-values adjustment methods. unadjusted p-values quite small, many adjusted p-values tend similar unadjusted ones two adjustment methods also tend produce similar p-values.","code":"#Third component: Summary dist_rmc_mat$summary #>            measure1         measure2  df  rmcorr.r    lowerCI   upperCI #> 1    Blindwalk Away Blindwalk Toward 175 0.8065821 0.74808182 0.8526427 #> 2    Blindwalk Away  Triangulated BW 174 0.2382857 0.09366711 0.3730565 #> 3    Blindwalk Away           Verbal 175 0.7355813 0.65965209 0.7966468 #> 4    Blindwalk Away  Visual matching 174 0.7758245 0.70930425 0.8286489 #> 5  Blindwalk Toward  Triangulated BW 176 0.2254866 0.08109132 0.3606114 #> 6  Blindwalk Toward           Verbal 177 0.7160551 0.63619996 0.7807308 #> 7  Blindwalk Toward  Visual matching 177 0.7575109 0.68718940 0.8137687 #> 8   Triangulated BW           Verbal 178 0.1835838 0.03835025 0.3212218 #> 9   Triangulated BW  Visual matching 177 0.2537431 0.11120971 0.3860478 #> 10           Verbal  Visual matching 179 0.7341831 0.65888265 0.7949162 #>          p.vals effective.N #> 1  8.228992e-42         177 #> 2  1.449081e-03         176 #> 3  2.056415e-31         177 #> 4  1.226384e-36         176 #> 5  2.476132e-03         178 #> 6  1.937983e-29         179 #> 7  1.302874e-34         179 #> 8  1.362964e-02         180 #> 9  6.095365e-04         179 #> 10 6.400493e-32         181  #p-values only dist_rmc_mat$summary$p.vals #>  [1] 8.228992e-42 1.449081e-03 2.056415e-31 1.226384e-36 2.476132e-03 #>  [6] 1.937983e-29 1.302874e-34 1.362964e-02 6.095365e-04 6.400493e-32  #Vector of original, unadjusted p-values for all 10 comparisons p.vals <- dist_rmc_mat$summary$p.vals  p.vals.bonferroni <- p.adjust(p.vals,                                method = \"bonferroni\",                               n = length(p.vals))  p.vals.fdr <- p.adjust(p.vals,                         method = \"fdr\",                        n = length(p.vals))  #All p-values together all.pvals <- cbind(p.vals, p.vals.bonferroni, p.vals.fdr) colnames(all.pvals) <- c(\"Unadjusted\", \"Bonferroni\", \"fdr\") round(all.pvals, digits = 5) #>       Unadjusted Bonferroni     fdr #>  [1,]    0.00000    0.00000 0.00000 #>  [2,]    0.00145    0.01449 0.00181 #>  [3,]    0.00000    0.00000 0.00000 #>  [4,]    0.00000    0.00000 0.00000 #>  [5,]    0.00248    0.02476 0.00275 #>  [6,]    0.00000    0.00000 0.00000 #>  [7,]    0.00000    0.00000 0.00000 #>  [8,]    0.01363    0.13630 0.01363 #>  [9,]    0.00061    0.00610 0.00087 #> [10,]    0.00000    0.00000 0.00000"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jonathan Z. Bakdash. Author. Laura R. Marusich. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bakdash J, Marusich L (2024). rmcorr: Repeated Measures Correlation. R package version 0.7.0, https://lmarusich.github.io/rmcorr/, https://github.com/lmarusich/rmcorr.","code":"@Manual{,   title = {rmcorr: Repeated Measures Correlation},   author = {Jonathan Z. Bakdash and Laura R. Marusich},   year = {2024},   note = {R package version 0.7.0, https://lmarusich.github.io/rmcorr/},   url = {https://github.com/lmarusich/rmcorr}, }"},{"path":"/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Repeated Measures Correlation","text":"","code":"library(rmcorr) rmcorr(Subject, PaCO2, pH, bland1995) #> Warning in rmcorr(Subject, PaCO2, pH, bland1995): 'Subject' coerced into a #> factor #>  #> Repeated measures correlation #>  #> r #> -0.5067697 #>  #> degrees of freedom #> 38 #>  #> p-value #> 0.0008471081 #>  #> 95% confidence interval #> -0.7067146 -0.2318631"},{"path":"/index.html","id":"graphical-interface-for-rmcorr","dir":"","previous_headings":"","what":"Graphical Interface for rmcorr","title":"Repeated Measures Correlation","text":"Shiny web standalone app graphical user interface also available: Web app Standalone app","code":""},{"path":"/index.html","id":"disclaimer","dir":"","previous_headings":"","what":"Disclaimer","title":"Repeated Measures Correlation","text":"software developed U.S. Government employees subject revision. views expressed documentation corresponding publications necessary represent views U.S. Government. software provided meet need timely best science. warranty, expressed implied, made U.S. Government functionality software related material shall fact release constitute warranty. software provided condition U.S. Government shall held liable damages resulting authorized unauthorized use software. text adapted USGS software disclaimer: toxEval U.S. Government disclaimer","code":""},{"path":"/reference/HCAHPS2022.html","id":null,"dir":"Reference","previous_headings":"","what":"Nested and multivariate survey measures of hospital patient experience and other measures — HCAHPS2022","title":"Nested and multivariate survey measures of hospital patient experience and other measures — HCAHPS2022","text":"summary dataset non-independent units analysis (six regions nesting 50 U.S. states 3 U.S. territories) multivariate (composite) measures. survey assessing patient experience hospitalized care, Hospital Consumer Assessment Healthcare Providers Systems (HCAHPS) Survey; also referred CAHPS® Hospital Survey. data publicly released April 2023 U.S. Centers Medicare & Medicaid Services (CMS). HCAHPS standardized validated survey instrument evaluating patient experience. Patient experience indicator healthcare quality defined \"... range interactions patients healthcare system, including care health plans, doctors, nurses, staff hospitals...\"  https://web.archive.org/web/20230206233908/https://www.ahrq.gov/cahps/-cahps/patient-experience/index.html. HCAHPS composite measures consist multiple questions , , top box scores (see https://www.hcahpsonline.org/en/summary-analyses/). addition patient experience, additional measures whether hospital recommended , number participating hospitals, survey response rate Note representative sample Measures averaged state/territory level Respondents discharged hospital July 2021 July 2022 Results patient-mix adjusted, see https://doi.org/10.1111/j.1475-6773.2008.00914.x Additional Information: details data questions comprising composite measures, see https://www.hcahpsonline.org/globalassets/hcahps/star-ratings/tech-notes/april_2023_star-ratings_tech_notes.pdf specific questions HCAHPS survey, see https://www.hcahpsonline.org/globalassets/hcahps/quality-assurance/2023_survey-instruments_english_mail.pdf CAHPS® registered trademark U.S. Agency Healthcare Research Quality: https://www.ahrq.gov/cahps/-cahps/using-cahps-name/index.html","code":""},{"path":"/reference/HCAHPS2022.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nested and multivariate survey measures of hospital patient experience and other measures — HCAHPS2022","text":"","code":"HCAHPS2022"},{"path":"/reference/HCAHPS2022.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Nested and multivariate survey measures of hospital patient experience and other measures — HCAHPS2022","text":"data frame 53 rows 14 columns","code":""},{"path":"/reference/HCAHPS2022.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Nested and multivariate survey measures of hospital patient experience and other measures — HCAHPS2022","text":"CAHPS Hospital Survey (2022). HCAHPS Survey Results Table (Dataset) https://www.hcahpsonline.org/globalassets/hcahps/summary-analyses/summary-results/april-2023-public-report-july-2021---june-2022-discharges.pdf","code":""},{"path":"/reference/bland1995.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeated measurements of intramural pH and PaCO2 — bland1995","title":"Repeated measurements of intramural pH and PaCO2 — bland1995","text":"dataset containing repeated measurements intramural pH PaCO2 eight subjects, Bland & Altman (1995).","code":""},{"path":"/reference/bland1995.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeated measurements of intramural pH and PaCO2 — bland1995","text":"","code":"bland1995"},{"path":"/reference/bland1995.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Repeated measurements of intramural pH and PaCO2 — bland1995","text":"data frame 47 rows 3 variables","code":""},{"path":"/reference/bland1995.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Repeated measurements of intramural pH and PaCO2 — bland1995","text":"Bland, J.M., & Altman, D.G. (1995). Calculating correlation coefficients repeated observations: Part 1 – correlation within subjects. BMJ, 310, 446, doi:10.1136/bmj.310.6977.446","code":""},{"path":"/reference/geom_rmc.html","id":null,"dir":"Reference","previous_headings":"","what":"geom_rmc: ggplot2 geom for simplified graphing — geom_rmc","title":"geom_rmc: ggplot2 geom for simplified graphing — geom_rmc","text":"geom_rmc: ggplot2 geom simplified graphing","code":""},{"path":"/reference/geom_rmc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"geom_rmc: ggplot2 geom for simplified graphing — geom_rmc","text":"","code":"geom_rmc(rmc)"},{"path":"/reference/geom_rmc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"geom_rmc: ggplot2 geom for simplified graphing — geom_rmc","text":"rmc object class \"rmc\" generated rmcorr function.","code":""},{"path":[]},{"path":"/reference/geom_rmc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"geom_rmc: ggplot2 geom for simplified graphing — geom_rmc","text":"","code":"my.rmc <- rmcorr(participant = Subject, measure1 = PaCO2, measure2 = pH,                   dataset = bland1995) #> Warning: 'Subject' coerced into a factor                   ggplot2::ggplot(bland1995,       ggplot2::aes(x = PaCO2,                    y = pH,                    color = factor(Subject))) +      geom_rmc(my.rmc)                                       ##manually: ggplot2::ggplot(bland1995,       ggplot2::aes(x = PaCO2,                    y = pH,                    color = factor(Subject))) +     ggplot2::geom_point(ggplot2::aes(colour = factor(Subject))) +     ggplot2::geom_line(ggplot2::aes(y = my.rmc$model$fitted.values),                         linetype = 1)    ##another example: ##new theme, remove legend, and custom color pal ggplot2::ggplot(bland1995,                 ggplot2::aes(x = PaCO2,                              y = pH,                              color = factor(Subject))) +    geom_rmc(my.rmc) +    ggplot2::theme_minimal() +    ggplot2::theme(legend.position=\"none\") +    ggplot2::scale_color_brewer(palette=\"Dark2\")"},{"path":"/reference/gilden2010.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeated measurements of reaction time and accuracy — gilden2010","title":"Repeated measurements of reaction time and accuracy — gilden2010","text":"dataset containing four repeated measurements reaction time (RT) accuracy eleven subjects visual search experiment. measurement mean RT accuracy block 288 search trials. blocks visual search, eleven subjects.","code":""},{"path":"/reference/gilden2010.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeated measurements of reaction time and accuracy — gilden2010","text":"","code":"gilden2010"},{"path":"/reference/gilden2010.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Repeated measurements of reaction time and accuracy — gilden2010","text":"data frame 44 rows 4 variables","code":""},{"path":"/reference/gilden2010.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Repeated measurements of reaction time and accuracy — gilden2010","text":"Gilden, D.L., Thornton, T.L., & Marusich, L.R. (2010). serial process visual search. Journal Experimental Psychology: Human Perception Performance, 36, 533-542, doi:10.1037/a0016464","code":""},{"path":"/reference/marusich2016_exp2.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeated measurements of dyads performance and subjective situation awareness — marusich2016_exp2","title":"Repeated measurements of dyads performance and subjective situation awareness — marusich2016_exp2","text":"dataset containing three repeated measures dyads (paired participants) working together capture High Value Targets (lower task time better performance) averaged Mission Awareness Rating Scale (MARS) score block, repeated three times. MARS evaluates subjective situation awareness (”knowing going ”), higher values indicate better situation awareness.","code":""},{"path":"/reference/marusich2016_exp2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeated measurements of dyads performance and subjective situation awareness — marusich2016_exp2","text":"","code":"marusich2016_exp2"},{"path":"/reference/marusich2016_exp2.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Repeated measurements of dyads performance and subjective situation awareness — marusich2016_exp2","text":"data frame 84 rows (28 dyads/pairs) 4 variables","code":""},{"path":"/reference/marusich2016_exp2.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Repeated measurements of dyads performance and subjective situation awareness — marusich2016_exp2","text":"Marusich et al. (2016). Effects information availability command--control decision making: performance, trust, situation awareness. Human Factors, 58(2), 301-321, doi:10.1177/0018720815619515","code":""},{"path":"/reference/plot.rmc.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the repeated measures correlation coefficient. — plot.rmc","title":"Plot the repeated measures correlation coefficient. — plot.rmc","text":"plot.rmc  produces scatterplot measure1 x-axis measure2 y-axis, different color used subject. Parallel lines fitted subject's data.","code":""},{"path":"/reference/plot.rmc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the repeated measures correlation coefficient. — plot.rmc","text":"","code":"# S3 method for class 'rmc' plot(x, palette = NULL, xlab = NULL, ylab = NULL, ...)"},{"path":"/reference/plot.rmc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the repeated measures correlation coefficient. — plot.rmc","text":"x object class \"rmc\" generated rmcorr function. palette palette used. Defaults RColorBrewer \"Paired\" palette xlab label x axis, defaults variable name measure1. ylab label y axis, defaults variable name measure2. ... additional arguments plot.","code":""},{"path":[]},{"path":"/reference/plot.rmc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the repeated measures correlation coefficient. — plot.rmc","text":"","code":"## Bland Altman 1995 data my.rmc <- rmcorr(participant = Subject, measure1 = PaCO2, measure2 = pH,                   dataset = bland1995) #> Warning: 'Subject' coerced into a factor plot(my.rmc)   ## Raz et al. 2005 data my.rmc <- rmcorr(participant = Participant, measure1 = Age, measure2 =                   Volume, dataset = raz2005) #> Warning: 'Participant' coerced into a factor library(RColorBrewer) blueset <- brewer.pal(8, 'Blues') pal <- colorRampPalette(blueset) plot(my.rmc, overall = TRUE, palette = pal, overall.col = 'black')    ## Gilden et al. 2010 data my.rmc <- rmcorr(participant = sub, measure1 = rt, measure2 = acc,                   dataset = gilden2010) #> Warning: 'sub' coerced into a factor plot(my.rmc, overall = FALSE, lty = 2, xlab = \"Reaction Time\",       ylab = \"Accuracy\")"},{"path":"/reference/print.rmc.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the results of a repeated measures correlation — print.rmc","title":"Print the results of a repeated measures correlation — print.rmc","text":"Print results repeated measures correlation","code":""},{"path":"/reference/print.rmc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the results of a repeated measures correlation — print.rmc","text":"","code":"# S3 method for class 'rmc' print(x, ...)"},{"path":"/reference/print.rmc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the results of a repeated measures correlation — print.rmc","text":"x object class \"rmc\", result call rmcorr. ... additional arguments print.","code":""},{"path":[]},{"path":"/reference/print.rmc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print the results of a repeated measures correlation — print.rmc","text":"","code":"## Bland Altman 1995 data blandrmc <- rmcorr(Subject, PaCO2, pH, bland1995) #> Warning: 'Subject' coerced into a factor blandrmc #>  #> Repeated measures correlation #>  #> r #> -0.5067697 #>  #> degrees of freedom #> 38 #>  #> p-value #> 0.0008471081 #>  #> 95% confidence interval #> -0.7067146 -0.2318631  #>"},{"path":"/reference/print.rmcmat.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the repeated measures correlation matrix — print.rmcmat","title":"Print the repeated measures correlation matrix — print.rmcmat","text":"Print repeated measures correlation matrix","code":""},{"path":"/reference/print.rmcmat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the repeated measures correlation matrix — print.rmcmat","text":"","code":"# S3 method for class 'rmcmat' print(x, ...)"},{"path":"/reference/print.rmcmat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the repeated measures correlation matrix — print.rmcmat","text":"x object class \"rmcmat\", result call rmcorr_mat. ... additional arguments print.","code":""},{"path":[]},{"path":"/reference/print.rmcmat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print the repeated measures correlation matrix — print.rmcmat","text":"","code":"## Bland Altman 1995 data blandrmc <- rmcorr(Subject, PaCO2, pH, bland1995) #> Warning: 'Subject' coerced into a factor blandrmc #>  #> Repeated measures correlation #>  #> r #> -0.5067697 #>  #> degrees of freedom #> 38 #>  #> p-value #> 0.0008471081 #>  #> 95% confidence interval #> -0.7067146 -0.2318631  #>"},{"path":"/reference/raz2005.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeated measurements of age and cerebellar volume — raz2005","title":"Repeated measurements of age and cerebellar volume — raz2005","text":"dataset containing two repeated measures, two occasions (Time), age adjusted volume cerebellar hemispheres 72 participants. Data captured Figure 8, Cerebellar Hemispheres (lower right) Raz et al. (2005).","code":""},{"path":"/reference/raz2005.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeated measurements of age and cerebellar volume — raz2005","text":"","code":"raz2005"},{"path":"/reference/raz2005.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Repeated measurements of age and cerebellar volume — raz2005","text":"data frame 144 rows 4 variables","code":""},{"path":"/reference/raz2005.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Repeated measurements of age and cerebellar volume — raz2005","text":"Raz, N., Lindenberger, U., Rodrigue, K.M., Kennedy, K.M., Head, D., Williamson, ., Dahle, C., Gerstorf, D., & Acker, J.D. (2005). Regional brain changes aging healthy adults: General trends, individual differences, modifiers. Cerebral Cortex, 15, 1676-1689, doi:10.1093/cercor/bhi044","code":""},{"path":"/reference/rmcorr-package.html","id":null,"dir":"Reference","previous_headings":"","what":"A package for computing the repeated measures correlation coefficient — rmcorr-package","title":"A package for computing the repeated measures correlation coefficient — rmcorr-package","text":"Compute repeated measures correlation, statistical technique determining overall within-individual relationship among paired measures assessed two occasions, first introduced Bland Altman (1995). Includes functions diagnostics, p-value, effect size confidence interval including optional bootstrapping, well graphing. Also includes several example datasets. details, see web documentation https://lmarusich.github.io/rmcorr/index.html original paper: Bakdash Marusich (2017) doi:10.3389/fpsyg.2017.00456 . Compute repeated measures correlation, statistical technique     determining overall within-individual relationship among paired measures     assessed two occasions, first introduced Bland Altman (1995).     Includes functions diagnostics, p-value, effect size confidence     interval including optional bootstrapping, well graphing. Also includes     several example datasets. details, see web documentation     <https://lmarusich.github.io/rmcorr/index.html>     original paper: Bakdash Marusich (2017) <doi:10.3389/fpsyg.2017.00456>.","code":""},{"path":"/reference/rmcorr-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A package for computing the repeated measures correlation coefficient — rmcorr-package","text":"Bakdash, J.Z. & Marusich, L.R. (2017). Repeated Measures Correlation, Frontiers Psychology, 8, 456, doi:10.3389/fpsyg.2017.00456 Bakdash, J.Z. & Marusich, L.R. (2019). Corrigendum: Repeated Measures Correlation, Frontiers Psychology, 10, doi:10.3389/fpsyg.2019.01201","code":""},{"path":[]},{"path":"/reference/rmcorr-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"A package for computing the repeated measures correlation coefficient — rmcorr-package","text":"Maintainer: Laura R. Marusich lmarusich@gmail.com (ORCID) Authors: Jonathan Z. Bakdash jbakdash@gmail.com (ORCID)","code":""},{"path":"/reference/rmcorr.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the repeated measures correlation coefficient. — rmcorr","title":"Calculate the repeated measures correlation coefficient. — rmcorr","text":"Calculate repeated measures correlation coefficient.","code":""},{"path":"/reference/rmcorr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the repeated measures correlation coefficient. — rmcorr","text":"","code":"rmcorr(   participant,   measure1,   measure2,   dataset,   CI.level = 0.95,   CIs = c(\"analytic\", \"bootstrap\"),   nreps = 100,   bstrap.out = F )"},{"path":"/reference/rmcorr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the repeated measures correlation coefficient. — rmcorr","text":"participant variable giving subject name/id observation. measure1 numeric variable giving observations one measure. measure2 numeric variable giving observations second measure. dataset data frame containing variables. CI.level confidence level interval CIs method calculating confidence intervals. nreps number resamples take bootstrapping. bstrap.Determines output include bootstrap resamples.","code":""},{"path":"/reference/rmcorr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the repeated measures correlation coefficient. — rmcorr","text":"list class \"rmc\" containing following components. r value repeated measures correlation coefficient. df degrees freedom p p-value repeated measures correlation coefficient. CI 95% confidence interval repeated measures correlation coefficient. model multiple regression model used calculate correlation coefficient. resamples bootstrap resampled correlation values.","code":""},{"path":"/reference/rmcorr.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate the repeated measures correlation coefficient. — rmcorr","text":"Bakdash, J.Z., & Marusich, L.R. (2017). Repeated Measures Correlation. Frontiers Psychology, 8, 456, doi:10.3389/fpsyg.2017.00456 . Bakdash, J. Z., & Marusich, L. R. (2019). Corrigendum: Repeated Measures Correlation. Frontiers Psychology, 10, doi:10.3389/fpsyg.2019.01201 . Bland, J.M., & Altman, D.G. (1995a). Calculating correlation coefficients repeated observations: Part 1 – correlation within subjects. BMJ, 310, 446, doi:10.1136/bmj.310.6977.446 Bland, J.M., & Altman, D.G. (1995b). Calculating correlation coefficients repeated observations: Part 2 – correlation within subjects. BMJ, 310, 633, doi:10.1136/bmj.310.6980.633","code":""},{"path":[]},{"path":"/reference/rmcorr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the repeated measures correlation coefficient. — rmcorr","text":"","code":"## Bland Altman 1995 data rmcorr(Subject, PaCO2, pH, bland1995) #> Warning: 'Subject' coerced into a factor #>  #> Repeated measures correlation #>  #> r #> -0.5067697 #>  #> degrees of freedom #> 38 #>  #> p-value #> 0.0008471081 #>  #> 95% confidence interval #> -0.7067146 -0.2318631  #>"},{"path":"/reference/rmcorr_mat.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a repeated measures correlation matrix. — rmcorr_mat","title":"Create a repeated measures correlation matrix. — rmcorr_mat","text":"Create repeated measures correlation matrix.","code":""},{"path":"/reference/rmcorr_mat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a repeated measures correlation matrix. — rmcorr_mat","text":"","code":"rmcorr_mat(participant, variables, dataset, CI.level = 0.95)"},{"path":"/reference/rmcorr_mat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a repeated measures correlation matrix. — rmcorr_mat","text":"participant variable giving subject name/id observation. variables character vector indicating columns variables include correlation matrix. dataset data frame containing variables. CI.level level confidence intervals use rmcorr models.","code":""},{"path":"/reference/rmcorr_mat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a repeated measures correlation matrix. — rmcorr_mat","text":"list class \"rmcmat\" containing following components. matrix repeated measures correlation matrix summary dataframe showing rmcorr stats pair variables models list full rmcorr model pair variables","code":""},{"path":"/reference/rmcorr_mat.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create a repeated measures correlation matrix. — rmcorr_mat","text":"Bakdash, J.Z., & Marusich, L.R. (2017). Repeated Measures Correlation. Frontiers Psychology, 8, 456. doi:10.3389/fpsyg.2017.00456 . Bland, J.M., & Altman, D.G. (1995). Calculating correlation coefficients repeated observations: Part 1 – correlation within subjects. BMJ, 310, 446, doi:10.1136/bmj.310.6977.446 . Cohen, P., West, S. G., & Aiken, L. S. (2002). Applied multiple regression/correlation analysis behavioral sciences (3rd edition), Routledge. ISBN: 9780805822236.","code":""},{"path":[]},{"path":"/reference/rmcorr_mat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a repeated measures correlation matrix. — rmcorr_mat","text":"","code":"dist_rmc_mat <- rmcorr_mat(participant = Subject,                             variables = c(\"Blindwalk Away\",                                          \"Blindwalk Toward\",                                          \"Triangulated BW\",                                          \"Verbal\",                                          \"Visual matching\"),                            dataset = twedt_dist_measures,                            CI.level = 0.95) plot(dist_rmc_mat$models[[2]])"},{"path":"/reference/twedt_dist_measures.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeated measures and multivariate measures of perceived distance — twedt_dist_measures","title":"Repeated measures and multivariate measures of perceived distance — twedt_dist_measures","text":"dataset repeated measures distance perception physical distances 7, 8, 9, 10, 11 meters. data also multivariate, five dependent measures distance perception. 5 (physical distance) x 5 (dependent measure) within-participants design sample size 46. Note data missing 15 trials due participant experimenter errors.","code":""},{"path":"/reference/twedt_dist_measures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeated measures and multivariate measures of perceived distance — twedt_dist_measures","text":"","code":"twedt_dist_measures"},{"path":"/reference/twedt_dist_measures.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Repeated measures and multivariate measures of perceived distance — twedt_dist_measures","text":"data frame 230 rows 7 columns","code":""},{"path":"/reference/twedt_dist_measures.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Repeated measures and multivariate measures of perceived distance — twedt_dist_measures","text":"Twedt, E. Bakdash, J.Z., Proffitt, D.R. (2022). Repeated multivariate measures perceived distance (Dataset) doi:10.5281/zenodo.6967162","code":""},{"path":"/news/index.html","id":"rmcorr-070","dir":"Changelog","previous_headings":"","what":"rmcorr 0.7.0","title":"rmcorr 0.7.0","text":"Added geom simplified graphing ggplot2: geom_rmc Added vignette overfitting/pseudoreplication Updated roxygen 7.3.2 now using _PACKAGE","code":""},{"path":"/news/index.html","id":"rmcorr-060","dir":"Changelog","previous_headings":"","what":"rmcorr 0.6.0","title":"rmcorr 0.6.0","text":"CRAN release: 2023-08-09 Added HCAHPS2022 dataset description Added vignette showing diagnostic plots Added vignette synthetic dataset w/NaN estimates (thanks Shreya Ghosh) possible solution Added references vignettes using bibtex Split references documentation functions datasets","code":""},{"path":"/news/index.html","id":"rmcorr-0542-dev-release-only-not-on-cran","dir":"Changelog","previous_headings":"","what":"rmcorr 0.5.4.2 (dev release only, not on CRAN)","title":"rmcorr 0.5.4.2 (dev release only, not on CRAN)","text":"Documentation update: Remove TravisCI, add U.S. gov disclaimer, fix typos formatting","code":""},{"path":"/news/index.html","id":"rmcorr-0541-dev-release-only-not-on-cran","dir":"Changelog","previous_headings":"","what":"rmcorr 0.5.4.1 (dev release only, not on CRAN)","title":"rmcorr 0.5.4.1 (dev release only, not on CRAN)","text":"Added reproducible example bootstrapping Specify article order web docs","code":""},{"path":"/news/index.html","id":"rmcorr-054","dir":"Changelog","previous_headings":"","what":"rmcorr 0.5.4","title":"rmcorr 0.5.4","text":"CRAN release: 2022-12-13 Corrected df confidence intervals. thank Benjamin Zobel finding mistake","code":""},{"path":"/news/index.html","id":"rmcorr-053-dev-release-only-not-on-cran","dir":"Changelog","previous_headings":"","what":"rmcorr 0.5.3 (dev release only, not on CRAN)","title":"rmcorr 0.5.3 (dev release only, not on CRAN)","text":"Added unit tests Use covr evaluate code coverage Documentation updates","code":""},{"path":"/news/index.html","id":"rmcorr-052","dir":"Changelog","previous_headings":"","what":"rmcorr 0.5.2","title":"rmcorr 0.5.2","text":"CRAN release: 2022-08-25 R 4.1.0 now required","code":""},{"path":"/news/index.html","id":"rmcorr-051-dev-release-only-not-on-cran","dir":"Changelog","previous_headings":"","what":"rmcorr 0.5.1 (dev release only, not on CRAN)","title":"rmcorr 0.5.1 (dev release only, not on CRAN)","text":"Documentation update: Table describing twedt_dist_measures now markdown chunk. renders correctly pkgdown","code":""},{"path":"/news/index.html","id":"rmcorr-050","dir":"Changelog","previous_headings":"","what":"rmcorr 0.5.0","title":"rmcorr 0.5.0","text":"CRAN release: 2022-08-08 Beta: Added rmcorr_mat() calculate rmcorr correlation matrix demonstrate rmcorr_mat(), added new dataset description: twedt_dist_measures Rewrote variable names saved rmcorr() Updated description: Minimum version R >3.5.0 serialized objects","code":""},{"path":"/news/index.html","id":"rmcorr-047-dev-release-only-not-on-cran","dir":"Changelog","previous_headings":"","what":"rmcorr 0.4.7 (dev release only, not on CRAN)","title":"rmcorr 0.4.7 (dev release only, not on CRAN)","text":"Fixed typo bland1995 example data: PacO2 now PaCO2 (partial pressure CO2) Add marusich2016 data description Documentation: Add details bland1995 data add DOIs data references","code":""},{"path":"/news/index.html","id":"rmcorr-046","dir":"Changelog","previous_headings":"","what":"rmcorr 0.4.6","title":"rmcorr 0.4.6","text":"CRAN release: 2022-05-02 use packages ‘suggests’ conditionally","code":""},{"path":"/news/index.html","id":"rmcorr-045","dir":"Changelog","previous_headings":"","what":"rmcorr 0.4.5","title":"rmcorr 0.4.5","text":"CRAN release: 2021-12-01 Fix issue column names Add testing","code":""},{"path":"/news/index.html","id":"rmcorr-044","dir":"Changelog","previous_headings":"","what":"rmcorr 0.4.4","title":"rmcorr 0.4.4","text":"CRAN release: 2021-08-10 Just changing maintainer email address","code":""},{"path":"/news/index.html","id":"rmcorr-043","dir":"Changelog","previous_headings":"","what":"rmcorr 0.4.3","title":"rmcorr 0.4.3","text":"CRAN release: 2021-04-01 Updated vignette","code":""},{"path":"/news/index.html","id":"rmcorr-042","dir":"Changelog","previous_headings":"","what":"rmcorr 0.4.2","title":"rmcorr 0.4.2","text":"CRAN release: 2021-03-30 Fixed another error bootstrapped confidence intervals Updated vignette","code":""},{"path":"/news/index.html","id":"rmcorr-041","dir":"Changelog","previous_headings":"","what":"rmcorr 0.4.1","title":"rmcorr 0.4.1","text":"CRAN release: 2020-08-26 Fixed error last update bootstrapped confidence intervals","code":""},{"path":"/news/index.html","id":"rmcorr-040","dir":"Changelog","previous_headings":"","what":"rmcorr 0.4.0","title":"rmcorr 0.4.0","text":"CRAN release: 2020-06-27 Added argument confidence level Reset contrast option running rmcorr Added parent.frame access","code":""},{"path":"/news/index.html","id":"rmcorr-030","dir":"Changelog","previous_headings":"","what":"rmcorr 0.3.0","title":"rmcorr 0.3.0","text":"CRAN release: 2018-02-28 column names can entered strings dynamically dataset parameter longer required plot.rmc function Added NEWS.md file track changes package.","code":""}]
